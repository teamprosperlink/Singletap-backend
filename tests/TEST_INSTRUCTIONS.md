# ğŸ§ª API Extraction Testing Guide

## ğŸ“‹ Overview

Test the GPT API extraction against expected outputs from `stage3_extraction1.json`.

**What we're testing:**
- Query â†’ Structured JSON (14 fields)
- Comparing actual output vs expected output
- Measuring accuracy and identifying issues

---

## ğŸš€ Quick Start

### 1. Setup API Key

Create `.env` file (copy from `.env.template`):
```bash
cp .env.template .env
```

Edit `.env` and add your OpenAI API key:
```
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
```

### 2. Install Dependencies

```bash
pip install openai python-dotenv
```

---

## ğŸ§ª Testing Approaches

### **Option 1: Test Single Query** (Recommended for debugging)

Test one query at a time:

```bash
python test_single_query.py
```

Or test custom query:
```bash
python test_single_query.py "need a yoga instructor in koramangala"
```

**Output:**
- Extracted JSON
- Token usage statistics
- Immediate feedback

---

### **Option 2: Full Test Suite - Single Stage**

One API call for full extraction:

```bash
python test_extraction_api.py single
```

**How it works:**
- Loads all 10 test queries
- One API call per query (full extraction)
- Compares actual vs expected
- Generates `test_results_single.json`

**Pros:**
- âœ… Simpler (one API call)
- âœ… Faster execution
- âœ… Lower latency

**Cons:**
- âš ï¸ Long prompt (~100K chars)
- âš ï¸ Might miss nuances

---

### **Option 3: Full Test Suite - Two Stage**

Two API calls: Classification â†’ Extraction:

```bash
python test_extraction_api.py two-stage
```

**How it works:**
1. **Stage 2 API call**: Classification only (intent, subintent, domain)
2. **Stage 3 API call**: Full extraction using Stage 2 context

**Pros:**
- âœ… Cleaner separation
- âœ… Easier debugging (know which stage fails)
- âœ… Matches production pipeline

**Cons:**
- âš ï¸ Two API calls (higher cost)
- âš ï¸ Higher latency

---

## ğŸ“Š Output Files

After running tests, you'll get:

### `test_results_single.json`
```json
[
  {
    "query": "looking for a used macbook...",
    "success": true,
    "actual": { /* extracted output */ },
    "differences": []
  },
  ...
]
```

### Console Output
```
ğŸ§ª EXTRACTION API TEST SUITE (SINGLE approach)
================================================================================

ğŸ“‚ Loading files...
âœ… Loaded prompt (300000 chars)
âœ… Loaded 10 test examples

Test 1/10: looking for a used macbook pro with at least 16gb ram...
================================================================================
ğŸ”¹ Single API call for full extraction...
âœ… PASS - Output matches expected

...

ğŸ“Š TEST SUMMARY
================================================================================
âœ… Passed: 8/10
âŒ Failed: 2/10
ğŸ“ˆ Success Rate: 80.0%

ğŸ’¾ Results saved to: test_results_single.json
```

---

## ğŸ” Understanding Results

### âœ… PASS
- All 13 fields match (excluding reasoning)
- Reasoning field skipped (non-deterministic)

### âŒ FAIL
- Shows which fields differ
- Expected vs Actual comparison
- Common issues:
  - Item type canonicalization (laptop vs notebook)
  - Attribute placement (min vs range)
  - Domain selection differences

---

## ğŸ› Debugging Failed Tests

If test fails, check:

1. **Field Differences**
   ```
   âŒ FAIL - Differences found:
      â€¢ items: value_mismatch
        Expected: [{"type": "laptop", ...}]
        Actual: [{"type": "notebook", ...}]
   ```

2. **Common Issues:**
   - **Item type:** "laptop" vs "notebook" â†’ Need canonicalization
   - **Constraint type:** Using `min` instead of `range` for exact values
   - **Domain:** "Technology & Electronics" vs "IT Services"
   - **Missing fields:** Check if API returned all 14 fields

3. **Solutions:**
   - Add examples to prompt
   - Use post-canonicalization layer (synonyms.json)
   - Adjust prompt instructions for specific issue

---

## ğŸ’° Cost Estimation

**GPT-4o Pricing (Nov 2024):**
- Prompt: $2.50 / 1M tokens
- Completion: $10.00 / 1M tokens

**Estimated cost per query:**
- Prompt: ~100K tokens â‰ˆ $0.25
- Completion: ~500 tokens â‰ˆ $0.005
- **Total: ~$0.255 per query**

**Full test suite (10 queries):**
- Single stage: ~$2.55
- Two stage: ~$5.10 (double calls)

---

## ğŸ“ˆ Next Steps

After testing:

1. **Analyze results** â†’ Identify patterns in failures
2. **Iterate on prompt** â†’ Fix common issues
3. **Add post-processing** â†’ Canonicalization layer
4. **Fine-tune Mistral** â†’ Use API-generated data for training
5. **Deploy on Azure** â†’ Production-ready model

---

## ğŸ¯ Success Metrics

**Target:**
- âœ… 90%+ field-level accuracy
- âœ… 100% schema compliance (all 14 fields present)
- âœ… Deterministic canonicalization (post-processing)

**Current challenges:**
- Item type variations (laptop/notebook/computer)
- Constraint type selection (min/max/range)
- Domain ambiguity (multi-domain queries)

**Solutions available:**
- `/data/synonyms.json` â†’ Canonical forms
- `/pos/data/linguistic_cues.json` â†’ Constraint detection
- `/pos/data/attributes_schema.json` â†’ Attribute classification

---

## ğŸ“ Troubleshooting

### API Key Issues
```
âŒ ERROR: OPENAI_API_KEY not found
```
â†’ Check `.env` file exists and has correct key

### Import Errors
```
ModuleNotFoundError: No module named 'openai'
```
â†’ Run: `pip install openai python-dotenv`

### JSON Decode Errors
```
JSONDecodeError: Expecting value: line 1 column 1
```
â†’ API returned non-JSON. Check model output format setting.

### Rate Limits
```
RateLimitError: Rate limit reached
```
â†’ Add delay between requests or upgrade API plan

---

## ğŸ“ Test Examples

The 10 test queries cover:

1. âœ… Product buy (used laptop with constraints)
2. âœ… Product sell (motorcycle with ownership)
3. âœ… Service seek (plumber with language preference)
4. âœ… Service provide (graphic designer with experience)
5. âœ… Mutual adventure (trekking buddy)
6. âœ… Mutual roommate (apartment with preferences)
7. âœ… Product free (giving away sofa)
8. âœ… Service education (math tutor)
9. âœ… Mutual professional (cofounder search)
10. âœ… Product simple (iPhone query)

---

## ğŸ”„ Iterative Testing Workflow

```
1. Run test_single_query.py with one example
   â†“
2. Inspect output, identify issues
   â†“
3. Adjust prompt or add examples
   â†“
4. Run full test suite
   â†“
5. Analyze success rate
   â†“
6. If < 90%: Iterate on prompt
   If â‰¥ 90%: Add post-processing layer
   â†“
7. Generate training data for fine-tuning
```

---

Happy testing! ğŸš€
