GLOBAL_REFERENCE_CONTEXT.md

# VRIDDHI â€” GLOBAL REFERENCE CONTEXT# (READ-ONLY Â· NEVER EDITED Â· INJECTED INTO EVERY STAGE)


### domains must follow these don't invent newer domains try to fit in this

----------------------------------------------------------------
DOMAIN FORMAT RULES (CRITICAL FOR MATCHING)
----------------------------------------------------------------

EXACT FORMAT: Use LOWERCASE with & symbol (for deterministic matching)
- âœ… "pets & animals" (CORRECT - lowercase, full name)
- âœ… "technology & electronics" (CORRECT - lowercase)
- âŒ "pets" (WRONG - use full domain name)
- âŒ "Pets & Animals" (WRONG - use lowercase)
- âŒ "Pets and Animals" (WRONG - use & not "and")

Output format: Array with exact lowercase string
- âœ… domain: ["pets & animals"]
- âœ… domain: ["technology & electronics"]
- âŒ domain: ["Technology & Electronics"]
- âŒ domain: ["pets"]

10. FIXED DOMAIN/CATEGORY LISTS

### 21 Product Domains

1. Technology & Electronics
2. Healthcare & Wellness
3. Fashion & Apparel
4. Home & Furniture
5. Food & Beverage
6. Automotive & Vehicles
7. Sports & Outdoors
8. Office & Stationery
9. Books, Media & Entertainment
10. Pets & Animals
11. Real Estate & Property
12. Manufacturing & Production
13. Agriculture & Farming
14. Environmental & Sustainability
15. Textile & Clothing Manufacturing
16. Jewelry & Accessories Manufacturing
17. Beauty & Cosmetics
18. Handicrafts & Artisan Products
19. Energy & Utilities
20. Security & Safety
21. Mining & Quarrying

### 18 Service Domains

1. Education & Training
2. Finance, Insurance & Legal
3. Transportation & Logistics
4. Hospitality, Travel & Accommodation
5. Business Services & Consulting
6. Marketing, Advertising & Design
7. Construction & Trades
8. Entertainment & Events
9. Personal Services
10. Government & Regulatory
11. Utilities & Infrastructure
12. Telecommunication & Internet
13. Nonprofit & Charity Services
14. Repair & Maintenance Services
15. Customs & Culture Services
16. Alternative & Holistic Health
17. Research & Development
18. Government & Public Administration

### 25 Mutual Categories

1. Housing
2. Roommates
3. Fitness
4. Sports
5. Partners
6. Travel
7. Adventure
8. Learning
9. Study
10. Professional
11. Career
12. Social
13. Friendship
14. Dating
15. Relationships
16. Parenting
17. Family
18. Hobbies
19. Interests
20. Pets
21. Animals
22. Support
23. Caregiving
24. Community
25. Volunteering

---

## INTERNAL CLASSIFICATION GUIDANCE (NEVER OUTPUT)

âš ï¸ **INTERNAL USE ONLY â€” NEVER INCLUDE IN OUTPUT OR REASONING FIELD**

These decision trees are scaffolding for classification. They guide the model to select the correct domain/category but MUST NEVER appear in any output field, especially not in `reasoning`.

---

### Chain of Thought (CoT) for Building the Service Domain Decision Tree

Start with broad economic sector split â€” Services are mostly tertiary (intangible value delivery). Distinguish public/government vs. private/market-driven first, as public services have unique authority/regulation aspects (domains 10, 18).

Handle physical vs. intangible â€” Physical/infrastructure/construction (7, 11, 14) vs. knowledge/advice (2, 5, 6, 17) vs. people-care (1, 9, 16) vs. movement/connectivity (3, 12) vs. experience/leisure (4, 8).

Prioritize primary purpose â€” Ask "What is the core value delivered?" (e.g., learning â†’ education; money/risk management â†’ finance/legal; healing/wellness â†’ health).

Resolve listed overlaps:
- Domains 10 & 18: Merge functionally (regulatory is a subset/tool of public administration); treat as one unless strictly enforcement vs. operations.
- Professional cluster (5, 6, 17): Differentiate by creative/output (marketing/design), advisory/operations (consulting/business), or innovation/knowledge creation (R&D).
- Health: Mainstream education/training vs. alternative/holistic (non-medical wellness).
- Repair vs. Construction: New/create vs. fix/maintain.
- Personal vs. Repair: Human body/appearance vs. objects/equipment.
- Culture vs. Entertainment: Tradition/heritage/ritual vs. leisure/amusement.

Make questions mutually exclusive where possible â€” Use binary or small-choice branches to guide quickly to one domain.

End leaves â€” Each terminal points to exactly one of the 18 domains (or notes rare hybrids).

Test mentally â€” Walk through examples (e.g., tax preparation â†’ Finance/legal; building permit office â†’ Government/regulatory or public admin; yoga studio â†’ Holistic health).

#### Service Domain Decision Tree (Text Flowchart)

Use this sequentially. Answer the question at each node and follow the branch.

```
Root: Is the service primarily provided by / for government entities, involves legal authority, regulation, compliance enforcement, or public policy execution?
â”œâ”€â”€ YES (Public / Government-oriented)
â”‚   â””â”€â”€ â†’ Merge 10 + 18 â†’ **Government & Regulatory** or **Government & Public Administration**
â”‚       (treat as equivalent; use "Government & Public Administration" if broad operations,
â”‚        "Government & Regulatory" if focused on rules/licensing/oversight)
â”‚
â””â”€â”€ NO (Primarily private, commercial, nonprofit, or individual market-driven)
    â”œâ”€â”€ Does it involve physical construction, installation, building, heavy trades, or large-scale infrastructure creation / major public works?
    â”‚   â”œâ”€â”€ YES â†’ **Construction & Trades** (7)
    â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Does it involve repair, fixing, servicing, or ongoing maintenance of objects, equipment, vehicles, homes, or appliances?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Repair & Maintenance Services** (14)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Does it involve moving people/goods, supply chain, warehousing, delivery, or fleet operations?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Transportation & Logistics** (3)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Does it provide core utilities (electricity, gas, water, sewage) or maintain large public infrastructure networks (grids, pipelines, telecom backbone)?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Utilities & Infrastructure** (11)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core offering connectivity, data transmission, internet access, mobile/cable services, or telecom infrastructure?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Telecommunication & Internet** (12)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value learning, skill-building, certification, academic instruction, or formal training programs?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Education & Training** (1)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value wellness, energy work, natural therapies, mind-body practices, or non-mainstream healing (not licensed medical)?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Alternative & Holistic Health** (16)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value money management, risk protection, investments, lending, accounting, taxes, contracts, or legal representation/advice?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Finance, Insurance & Legal** (2)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value strategic/management advice, operations improvement, HR, process optimization for organizations?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Business Services & Consulting** (5)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value creative/promotional output â€” advertising campaigns, branding, graphic/web design, copywriting, media buying?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Marketing, Advertising & Design** (6)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value new knowledge creation, experimentation, innovation, scientific/technical investigation, prototyping (not routine consulting)?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Research & Development** (17)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value temporary stay, lodging, food service, tourism planning, guided travel experiences?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Hospitality, Travel & Accommodation** (4)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value performances, shows, festivals, parties, recreation, amusement, sports events (primarily for enjoyment)?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Entertainment & Events** (8)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value personal appearance, grooming, body care, domestic help, pet care, individual lifestyle assistance?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Personal Services** (9)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Is the core value tradition, rituals, heritage preservation, cultural education, indigenous practices, community customs?
    â”‚       â”‚   â”œâ”€â”€ YES â†’ **Customs & Culture Services** (15)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â””â”€â”€ Is the core value social good, advocacy, relief, community support, donations, mission-driven without profit primary motive?
    â”‚           â””â”€â”€ YES â†’ **Nonprofit & Charity Services** (13)
    â””â”€â”€ (If still unclassified after all branches â€” rare hybrid; choose closest primary value or note multiple domains)
```

#### How to Use the Service Tree

- Walk top-down; most services reach a leaf in 4â€“8 questions.
- For borderline cases, ask: "If I had to remove all secondary aspects, what remains the one essential deliverable?"
- Examples tested against tree:
  - Tax preparation service â†’ Finance path â†’ YES â†’ Domain 2
  - Building a house â†’ Construction path â†’ YES â†’ Domain 7
  - Fixing a broken AC unit â†’ Repair path â†’ YES â†’ Domain 14
  - Yoga / Reiki studio â†’ Holistic health path â†’ YES â†’ Domain 16
  - Corporate strategy consulting â†’ Business consulting path â†’ YES â†’ Domain 5
  - Ad agency creating campaigns â†’ Marketing path â†’ YES â†’ Domain 6
  - Biotech lab inventing new drug â†’ R&D path â†’ YES â†’ Domain 17
  - Wedding DJ + planner â†’ Entertainment path â†’ YES â†’ Domain 8 (if fun-focused); Customs if heavily ritual/cultural
  - Passport office (government) â†’ Root YES â†’ Domain 10/18
  - Food bank distribution â†’ Nonprofit path â†’ Domain 13

---

### Chain of Thought (CoT) for Building the Product Domain Decision Tree

Observe the list structure â€” Mostly consumer/retail-oriented (1â€“10, 17, 20), with manufacturing/production (12, 15, 16, 18), raw/resource sectors (13, 21), sustainability (14), and infrastructure/energy (19, 11). Some overlaps: Healthcare & Wellness (2) vs. Beauty & Cosmetics (17); Home & Furniture (4) vs. Handicrafts (18); various manufacturing (12 general vs. specific like Textile 15).

Anchor to standard classifications â€” Draws inspiration from NAICS (e.g., Manufacturing, Retail Trade, Mining), UNSPSC top segments (e.g., Raw Materials, Apparel, Electronics), Google/Shopify product taxonomies (e.g., Apparel > Clothing, Home & Garden > Furniture), and e-commerce trends (fashion, beauty, food, electronics, sustainability rising in 2025â€“2026).

Prioritize primary nature â€” Consumer/end-user vs. B2B/production/raw vs. experiential/sustainable.

Resolve overlaps:
- Manufacturing: General (12) vs. specific (15 Textile, 16 Jewelry, 18 Handicrafts/Artisan).
- Wellness: Medical/health devices â†’ Healthcare (2); beauty/personal care â†’ Beauty (17).
- Home: Furniture/large durables â†’ Home & Furniture (4); artisan/decor â†’ Handicrafts (18).
- Energy/Utilities: Consumer products (e.g., solar panels) vs. raw extraction (Mining 21).
- Sustainability: Cross-cutting, but dedicated domain (14) for eco-products.

Make tree mutually exclusive â€” Start broad (consumer vs. production/raw), then drill by material/use/purpose.

End leaves â€” Point to exactly one of the 21 domains (rare hybrids noted).

Test examples â€” Smartphone â†’ Technology; Organic cotton shirt â†’ Textile Manufacturing or Fashion (primary end-use); Gold necklace â†’ Jewelry Manufacturing.

#### Product Domain Decision Tree (Text Flowchart)

Start at the root and follow branches based on the product's primary nature, intended use, and production level.

```
Root: Is the product primarily a raw material, extracted resource, agricultural output, or industrial-scale manufactured input/component (not finished consumer good)?
â”œâ”€â”€ YES (Raw / Production / B2B-oriented)
â”‚   â”œâ”€â”€ Extracted from earth (minerals, ores, coal, stone, aggregates)?
â”‚   â”‚   â””â”€â”€ YES â†’ **Mining & Quarrying** (21)
â”‚   â”œâ”€â”€ Grown/raised (crops, livestock, dairy, timber, fish farming)?
â”‚   â”‚   â””â”€â”€ YES â†’ **Agriculture & Farming** (13)
â”‚   â”œâ”€â”€ Manufactured at scale for other manufacturing (e.g., fabrics, yarns, threads, basic metals, chemicals, parts)?
â”‚   â”‚   â”œâ”€â”€ YES, and textile/fabric-based â†’ **Textile & Clothing Manufacturing** (15)
â”‚   â”‚   â”œâ”€â”€ YES, and jewelry/precision accessories (gems, metals, watches parts)?
â”‚   â”‚   â”‚   â””â”€â”€ YES â†’ **Jewelry & Accessories Manufacturing** (16)
â”‚   â”‚   â”œâ”€â”€ YES, and artisan/handmade/cultural craft items (not mass-produced)?
â”‚   â”‚   â”‚   â””â”€â”€ YES â†’ **Handicrafts & Artisan Products** (18)
â”‚   â”‚   â””â”€â”€ YES, general manufacturing/production (machinery, equipment, components, packaging)?
â”‚   â”‚       â””â”€â”€ â†’ **Manufacturing & Production** (12)
â”‚   â””â”€â”€ NO â†’ Re-evaluate (likely consumer; go to NO branch)
â”‚
â””â”€â”€ NO (Primarily finished consumer/retail product or end-user good)
    â”œâ”€â”€ Is it powered by/related to energy generation, distribution, or utilities (e.g., solar panels, batteries, fuel, power tools for energy)?
    â”‚   â””â”€â”€ YES â†’ **Energy & Utilities** (19)
    â”œâ”€â”€ Is it designed for protection, surveillance, defense, or safety (e.g., locks, alarms, helmets, fire extinguishers)?
    â”‚   â””â”€â”€ YES â†’ **Security & Safety** (20)
    â”œâ”€â”€ Is it focused on environmental protection, recycling, green/eco-friendly materials, or sustainability features as primary selling point?
    â”‚   â””â”€â”€ YES â†’ **Environmental & Sustainability** (14)
    â”œâ”€â”€ Is it electronic, digital, computing, gadgets, software/hardware, appliances with tech core?
    â”‚   â””â”€â”€ YES â†’ **Technology & Electronics** (1)
    â”œâ”€â”€ Is it health/medical devices, supplements, fitness equipment, therapeutic products?
    â”‚   â””â”€â”€ YES â†’ **Healthcare & Wellness** (2)
    â”œâ”€â”€ Is it personal care, makeup, skincare, fragrances, hair products?
    â”‚   â””â”€â”€ YES â†’ **Beauty & Cosmetics** (17)
    â”œâ”€â”€ Is it clothing, shoes, accessories for wear (not manufacturing input)?
    â”‚   â””â”€â”€ YES â†’ **Fashion & Apparel** (3)
    â”œâ”€â”€ Is it furniture, home decor, bedding, kitchenware, appliances for living spaces?
    â”‚   â””â”€â”€ YES â†’ **Home & Furniture** (4)
    â”œâ”€â”€ Is it food, drinks, groceries, ingredients, snacks, beverages?
    â”‚   â””â”€â”€ YES â†’ **Food & Beverage** (5)
    â”œâ”€â”€ Is it vehicles, parts, accessories for cars, bikes, trucks, motorcycles?
    â”‚   â””â”€â”€ YES â†’ **Automotive & Vehicles** (6)
    â”œâ”€â”€ Is it sports gear, exercise equipment, camping, outdoor adventure items?
    â”‚   â””â”€â”€ YES â†’ **Sports & Outdoors** (7)
    â”œâ”€â”€ Is it office supplies, desks, printers, paper, stationery, business tools?
    â”‚   â””â”€â”€ YES â†’ **Office & Stationery** (8)
    â”œâ”€â”€ Is it books, e-books, music, movies, games, streaming media physical/digital?
    â”‚   â””â”€â”€ YES â†’ **Books, Media & Entertainment** (9)
    â”œâ”€â”€ Is it pet food, toys, accessories, animal care products?
    â”‚   â””â”€â”€ YES â†’ **Pets & Animals** (10)
    â””â”€â”€ Is it property, land, buildings, real estate listings (not physical movable product)?
        â””â”€â”€ YES â†’ **Real Estate & Property** (11)
        (If no match after all â€” rare hybrid; choose primary consumer use or note multiple)
```

#### How to Use the Product Tree

- Most products reach a leaf quickly (3â€“7 questions).
- For borderline cases: Ask "What is the main end-user purchase reason?" or "If stripped to core identity, what sector claims it?"
- Quick test examples:
  - iPhone â†’ Technology & Electronics (1)
  - Yoga mat â†’ Sports & Outdoors (7) or Healthcare & Wellness (2) â†’ Wellness if therapeutic focus
  - Cotton fabric roll â†’ Textile & Clothing Manufacturing (15)
  - Handmade wooden sculpture â†’ Handicrafts & Artisan Products (18)
  - Electric car battery â†’ Energy & Utilities (19) or Automotive (6) â†’ Energy if power-focused
  - Sunscreen lotion â†’ Beauty & Cosmetics (17)
  - Organic farm tomatoes â†’ Agriculture & Farming (13) if raw; Food & Beverage (5) if packaged retail
  - Home security camera â†’ Security & Safety (20) or Technology (1) â†’ Security if protection primary

---

### Chain of Thought (CoT) for Building the Mutual Categories Decision Tree

Observe the list structure â€” These are primarily social/mutual connection categories (often used in apps like Bumble BFF, Meetup, Peanut, Nextdoor, or community platforms). They span living arrangements (1â€“2), physical activities (3â€“4), romantic/intimate (5,13â€“15), exploratory/experiential (6â€“7), educational/professional (8â€“11), platonic/social (12â€“13), familial/care (16â€“17,22â€“23), leisure/personal (18â€“21), and collective/altruistic (24â€“25).

Anchor to real-world usage â€” Draws from social/friendship/dating/community apps (e.g., Meetup groups by interest/activity, Bumble BFF modes, Peanut for parents, Nextdoor for neighbors/housing), sociology taxonomies (primary/secondary groups, interest vs. friendship groups), and relationship dimensions (e.g., permanence, intimacy level, voluntary vs. obligatory).

Prioritize primary connection type â€” Start with broad splits: Living/practical â†’ Physical/active â†’ Romantic/intimate â†’ Educational/career â†’ Platonic/social â†’ Familial/caregiving â†’ Leisure/personal â†’ Collective/community/altruistic.

Resolve overlaps:
- Friendship (13) vs. Social (12): Broader social vs. close personal friendship.
- Dating (14) vs. Relationships (15) vs. Partners (5): Initial romantic interest vs. established romantic vs. committed/long-term partner seeking.
- Learning (8) vs. Study (9): General skill/knowledge pursuit vs. formal/academic study.
- Hobbies (18) vs. Interests (19): Hands-on/doing activities vs. broader topics/passions.
- Parenting (16) vs. Family (17): Child-rearing specific vs. broader family connections.
- Pets (20) vs. Animals (21): Pet ownership/care vs. wildlife/animal interest.
- Support (22) vs. Caregiving (23): Emotional/peer support vs. hands-on caregiving.

Make tree mutually exclusive â€” Use binary/small-choice questions based on key attributes (intimacy level, activity type, obligation, formality).

End leaves â€” Each terminal points to exactly one of the 25 categories (note rare hybrids).

Test examples â€” "Find someone to hike with" â†’ Adventure/Sports; "Need a roommate" â†’ Roommates; "Want to talk about parenting challenges" â†’ Parenting/Support.

#### Mutual Categories Decision Tree (Text Flowchart)

Start at the root and follow branches sequentially.

```
Root: Is the primary goal or connection related to shared living space, cohabitation, or practical daily home arrangements?
â”œâ”€â”€ YES
â”‚   â”œâ”€â”€ Shared housing/property/room rental â†’ **Housing** (1)
â”‚   â””â”€â”€ Shared living with others (finding compatible cohabitants) â†’ **Roommates** (2)
â”‚
â””â”€â”€ NO (Primarily social, relational, activity-based, or emotional connection)
    â”œâ”€â”€ Is it centered on physical health, exercise, gym, body movement, or athletic performance?
    â”‚   â”œâ”€â”€ YES, competitive/team/individual sports/games â†’ **Sports** (4)
    â”‚   â””â”€â”€ YES, general fitness/wellness/training/yoga/running â†’ **Fitness** (3)
    â”‚
    â”œâ”€â”€ Is it romantic/sexual attraction, partnership, or intimate emotional bonding?
    â”‚   â”œâ”€â”€ YES, seeking initial romantic/sexual interest/matches â†’ **Dating** (14)
    â”‚   â”œâ”€â”€ YES, seeking committed long-term partner/spouse â†’ **Partners** (5)
    â”‚   â””â”€â”€ YES, ongoing established romantic/emotional connection â†’ **Relationships** (15)
    â”‚
    â”œâ”€â”€ Is it exploratory travel, trips, relocation, or cultural experiences?
    â”‚   â”œâ”€â”€ YES, leisure/vacation/exploration trips â†’ **Travel** (6)
    â”‚   â””â”€â”€ YES, high-risk/excitement/outdoor challenges (e.g., hiking, skydiving) â†’ **Adventure** (7)
    â”‚
    â”œâ”€â”€ Is it knowledge/skill acquisition or education-related?
    â”‚   â”œâ”€â”€ YES, formal/academic/classes/degrees/exams â†’ **Study** (9)
    â”‚   â””â”€â”€ YES, informal/self-directed learning/skills/workshops â†’ **Learning** (8)
    â”‚
    â”œâ”€â”€ Is it work/job/business/professional networking or advancement?
    â”‚   â”œâ”€â”€ YES, job search/mentoring/resume help â†’ **Career** (11)
    â”‚   â””â”€â”€ YES, professional networking/colleagues/business contacts â†’ **Professional** (10)
    â”‚
    â”œâ”€â”€ Is it platonic/non-romantic human connection?
    â”‚   â”œâ”€â”€ YES, broad casual socializing/events/meetups â†’ **Social** (12)
    â”‚   â””â”€â”€ YES, deeper personal/close friendship bonds â†’ **Friendship** (13)
    â”‚
    â”œâ”€â”€ Is it family-related or caregiving?
    â”‚   â”œâ”€â”€ YES, raising/parenting children/kids â†’ **Parenting** (16)
    â”‚   â”œâ”€â”€ YES, broader family ties/relatives/siblings â†’ **Family** (17)
    â”‚   â”œâ”€â”€ YES, emotional/mental support/advice/listening (peer or group) â†’ **Support** (22)
    â”‚   â””â”€â”€ YES, hands-on practical caregiving (elderly, disabled, sick) â†’ **Caregiving** (23)
    â”‚
    â”œâ”€â”€ Is it personal leisure, passion, or animal-related?
    â”‚   â”œâ”€â”€ YES, hands-on/doing activities (e.g., crafting, gaming, cooking) â†’ **Hobbies** (18)
    â”‚   â”œâ”€â”€ YES, topics/passions to discuss/share (e.g., movies, tech, philosophy) â†’ **Interests** (19)
    â”‚   â”œâ”€â”€ YES, pet ownership/care/playing with pets â†’ **Pets** (20)
    â”‚   â””â”€â”€ YES, wildlife/conservation/animals in general (not pets) â†’ **Animals** (21)
    â”‚
    â””â”€â”€ Is it group/community/altruistic involvement?
        â”œâ”€â”€ YES, local/neighborhood/belonging/voluntary groups â†’ **Community** (24)
        â””â”€â”€ YES, unpaid helping/charity/service to others â†’ **Volunteering** (25)
        (If no clear match â€” rare hybrid; choose primary intent, e.g., "parent support group" â†’ Parenting + Support)
```

#### How to Use the Mutual Tree

- Walk top-down; most reach a leaf in 4â€“8 questions.
- For borderline cases: Ask "What is the main emotional/practical outcome desired?" or "If forced to one core type of bond/activity, what is it?"
- Quick test examples:
  - "Find people for weekend hikes" â†’ Adventure (7) or Sports (4) â†’ Adventure if exploratory/outdoors-focused.
  - "Need someone to share apartment rent" â†’ Roommates (2).
  - "Looking for a life partner" â†’ Partners (5).
  - "Want casual coffee chats" â†’ Social (12) or Friendship (13) â†’ Social if broad.
  - "Discuss parenting tips" â†’ Parenting (16) or Support (22) â†’ Parenting if child-rearing primary.
  - "Join a book club" â†’ Interests (19) or Hobbies (18) â†’ Interests if discussion-focused.
  - "Help at animal shelter" â†’ Animals (21) or Volunteering (25) â†’ Volunteering if service primary.

---

### Read this very important 
## <field_name>

1. Definition
2. When this field MUST be populated
3. When this field MUST be empty
4. Allowed structure & data type
5. Standardization & normalization rules
6. What the model MUST do
7. What the model MUST NEVER do
8. Positive examples (TRUE POSITIVES)
9. Negative examples
   - Hard negatives
   - False positives
   - False negatives
10. Edge cases & ambiguity handling
11. Validation checks
No deviation. No creativity.

## INTENT
### SECTION A: INTENT CLASSIFICATION (Q1-Q5)

1. Definition

intent defines the fundamental nature of the userâ€™s request.

It answers WHAT kind of interaction the user wants, independent of domain, attributes, or constraints.

The system recognizes exactly three intents:

product | service | mutual

2. When this field MUST be populated

ALWAYS

intent is a mandatory field

No query is allowed to proceed without a resolved intent

If intent cannot be determined unambiguously â†’ sample must be REJECTED in Stage 4

3. When this field MUST be empty

NEVER

intent must never be empty, null, or missing

4. Allowed structure & data type
"intent": "product" | "service" | "mutual"


Type: string (enum)

Single value only

Case-insensitive at input, stored lowercase

No arrays

No alternative labels

No extensions

5. Standardization & normalization rules

Normalize all variants to exact enum values

Synonyms, wording, or phrasing do not affect output value

User expression	Normalized intent
buy, sell, phone, car	product
tutor, repair, design, teaching	service
partner, buddy, roommate	mutual

âš ï¸ Never invent new intent types

6. What the model MUST do

Decide intent semantically, not via keyword match

Use context and meaning, not word presence alone

Assign exactly one intent

Resolve ambiguity using intent priority rules (see Section 10)

7. What the model MUST NEVER do

ğŸš« Infer intent from domain alone
ğŸš« Create hybrid intents
ğŸš« Output multiple intents
ğŸš« Leave intent undefined
ğŸš« Emit reasoning steps or decision trees in output
ğŸš« Change enum values

8. INTERNAL CoT QUESTIONS (DATA GENERATION ONLY)

âš ï¸ IMPORTANT

These questions are INTERNAL scaffolding for data generation.

âŒ They MUST NOT appear in:

Training samples

Model outputs

reasoning field

âœ… They MAY be used by the generator to ensure consistency.

Q1: Is there a PRODUCT (tangible item, ownership transfer)?
    Signals: buy, sell, purchase, phone, car, laptop, furniture
    â†’ YES â†’ intent = product
    â†’ NO â†’ continue

Q2: Is there a SERVICE (work, expertise, task performed, no ownership)?
    Signals: need X person, -er professions, services, repair, consultation
    â†’ YES â†’ intent = service
    â†’ NO â†’ continue

Q3: Is there a MUTUAL activity (shared participation, symmetric roles)?
    Signals: partner, buddy, flatmate, companion, together, with me
    â†’ YES â†’ intent = mutual

Q4: If multiple signals exist:
    - Ownership transfer dominates â†’ product
    - Work performed dominates â†’ service
    - Symmetric relationship dominates â†’ mutual

Q5: If still unclear â†’ mark sample INVALID

9. Positive examples (TRUE POSITIVES)
Product
Query	intent
â€œlooking to buy an iphoneâ€	product
â€œselling my bikeâ€	product
â€œanyone selling used furnitureâ€	product
Service
Query	intent
â€œneed a plumberâ€	service
â€œlooking for math tutorâ€	service
â€œi offer graphic design servicesâ€	service
Mutual
Query	intent
â€œlooking for a gym buddyâ€	mutual
â€œneed a roommateâ€	mutual
â€œanyone want to travel togetherâ€	mutual
10. Negative examples
âŒ Hard Negatives (must be rejected or corrected)
Query	Wrong intent	Why
â€œselling my time to startupsâ€	product	Time is not a product
â€œbuying mentorship sessionsâ€	product	No ownership transfer
â€œhiring cofounderâ€	service	Cofounder is mutual
âŒ False Positives
Query	Incorrect	Correct
â€œlooking for a tennis partnerâ€ â†’ service	âŒ	mutual
â€œbuying consulting hoursâ€ â†’ product	âŒ	service
âŒ False Negatives
Query	Missed intent
â€œanyone up for morning walks?â€	mutual
â€œneed someone to fix my sinkâ€	service
11. Edge cases & ambiguity handling
Case 1: Product + Service mentioned together

â€œbuy a laptop and need help setting it upâ€

intent = product

service aspects handled later (ignored at intent stage)

Case 2: Mutual + Service ambiguity

â€œlooking for a cofounder to build a startupâ€

No payment

Symmetric roles
â†’ intent = mutual

Case 3: Vague phrasing

â€œneed help with my websiteâ€

No ownership transfer

Task performed
â†’ intent = service

Case 4: Multiple intents implied

â€œselling my camera and teaching photographyâ€

âš ï¸ Ambiguous
â†’ INVALID sample
(or split into two queries at Stage 1)

12. Validation checks

A sample is INVALID if:

intent âˆ‰ {product, service, mutual}

intent is missing or empty

Multiple intents implied without dominance

Intent inferred but not stated semantically

Output leaks CoT / decision steps

FINAL LOCK (DO NOT CHANGE)

Intent answers WHAT type of interaction exists.
It is decided once, first, and never revised downstream.

### SUB_INTENT
FIELD SPECIFICATION â€” subintent
1. Definition

subintent specifies the direction of action within a resolved intent.

It answers WHAT SIDE of the interaction the user is on (demand vs supply), after intent is fixed.

subintent is dependent on intent and cannot exist independently.

2. When this field MUST be populated

ALWAYS, once intent is resolved

Mandatory for all three intents

If intent exists and subintent is missing â†’ INVALID sample

3. When this field MUST be empty

NEVER

subintent must never be null, empty, or omitted

4. Allowed structure & data type
"subintent": "buy" | "sell" | "seek" | "provide" | "connect"


Type: string (enum)

Single value only

Lowercase only

No arrays

No aliases

No extensions

5. Standardization & normalization rules
Subintent mapping by intent (LOCKED)
intent	Allowed subintent(s)
product	buy, sell
service	seek, provide
mutual	connect

ğŸš« Any other combination is INVALID.

Direction normalization
User language	Normalized subintent
want, need, looking for	buy / seek
selling, offering, available	sell / provide
partner, buddy, together	connect
6. What the model MUST do

Assign exactly one subintent

Ensure subintent is compatible with intent

Decide direction semantically, not by keywords alone

Resolve tense, phrasing, and implied direction correctly

7. What the model MUST NEVER do

ğŸš« Assign multiple subintents
ğŸš« Use subintent values outside allowed enum
ğŸš« Infer supply when demand is stated
ğŸš« Change intentâ€“subintent pairing
ğŸš« Emit direction reasoning or decision trees in output

8. INTERNAL CoT QUESTIONS (DATA GENERATION ONLY)

âš ï¸ INTERNAL USE ONLY â€” NEVER OUTPUT

Q1: Is the user DEMANDING something?
    Signals: DON'T GO ON THE KEYWORDS UNDERSTAND THE EMOTION
    â†’ demand

Q2: Is the user OFFERING something?
    Signals: DON'T GO ON THE KEYWORDS UNDERSTAND THE EMOTION
    â†’ supply

Q3: Combine with intent:
    product + demand â†’ buy
    product + supply â†’ sell
    service + demand â†’ seek
    service + supply â†’ provide
    mutual â†’ connect (always)

Q4: If both demand and supply are present:
    â†’ SEE IF IT WAS EXCHANGE IT WILL GO TO MUTUAL

9. Positive examples (TRUE POSITIVES)
Product
Query	subintent
â€œlooking to buy a used iphoneâ€	buy
â€œselling my old laptopâ€	sell
Service
Query	subintent
â€œneed a math tutorâ€	seek
â€œi offer freelance designâ€	provide
Mutual
Query	subintent
â€œlooking for a gym buddyâ€	connect
â€œneed a cofounderâ€	connect
10. Negative examples
âŒ Hard Negatives
Case	Why invalid
product + seek	seek is not allowed for product
service + buy	buy is ownership transfer only
mutual + provide	mutual has no direction
âŒ False Positives
Query	Wrong	Correct
â€œselling consultation hoursâ€	sell	provide
â€œbuying tutoring sessionsâ€	buy	seek
âŒ False Negatives
Query	Missed subintent
â€œany tutors available?â€	seek
â€œdesigner here for freelance workâ€	provide
11. Edge cases & ambiguity handling
Case 1: Self-description implying supply

â€œiâ€™m a backend developer, open to projectsâ€

â†’ service + provide

Case 2: Question form implies demand

â€œany good plumbers around?â€

â†’ service + seek

Case 3: Mutual phrased as demand

â€œneed a roommateâ€

â†’ mutual + connect
(Direction is symmetric, not demand/supply)

Case 4: Both demand and supply present

â€œselling my camera and looking to buy anotherâ€

âš ï¸ INVALID SINGLE SAMPLE
Must be split upstream.

12. Validation checks

A sample is INVALID if:

subintent does not match allowed set for intent

subintent missing or null

Multiple directions implied

Direction inferred without semantic support

CoT / decision logic appears in output

FINAL LOCK (DO NOT CHANGE)

subintent defines direction, not desire.
It is strictly constrained by intent.
One intent â†’ one direction â†’ one subintent.

### DOMAIN 
FIELD SPECIFICATION â€” domain

"""
### 21 Product Domains

1. Technology & Electronics
2. Healthcare & Wellness
3. Fashion & Apparel
4. Home & Furniture
5. Food & Beverage
6. Automotive & Vehicles
7. Sports & Outdoors
8. Office & Stationery
9. Books, Media & Entertainment
10. Pets & Animals
11. Real Estate & Property
12. Manufacturing & Production
13. Agriculture & Farming
14. Environmental & Sustainability
15. Textile & Clothing Manufacturing
16. Jewelry & Accessories Manufacturing
17. Beauty & Cosmetics
18. Handicrafts & Artisan Products
19. Energy & Utilities
20. Security & Safety
21. Mining & Quarrying

### 18 Service Domains

1. Education & Training
2. Finance, Insurance & Legal
3. Transportation & Logistics
4. Hospitality, Travel & Accommodation
5. Business Services & Consulting
6. Marketing, Advertising & Design
7. Construction & Trades
8. Entertainment & Events
9. Personal Services
10. Government & Regulatory
11. Utilities & Infrastructure
12. Telecommunication & Internet
13. Nonprofit & Charity Services
14. Repair & Maintenance Services
15. Customs & Culture Services
16. Alternative & Holistic Health
17. Research & Development
18. Government & Public Administration
"""
1. Definition

domain defines the high-level problem space or market vertical to which the user query belongs.

It answers â€œWHAT general area is this request about?â€, independent of intent direction, attributes, or constraints.

domain does NOT describe how, how much, or with what qualities â€” only WHAT space.

2. When this field MUST be populated

ALWAYS

Mandatory for product, service, and mutual

At least one domain must be assigned

If no suitable domain exists â†’ choose the closest predefined domain
If still impossible â†’ INVALID sample

3. When this field MUST be empty

NEVER

domain cannot be empty, null, or missing

4. Allowed structure & data type
"domain": ["<domain_string>"]


Type: array of strings

Minimum length: 1

Maximum length: N (multi-domain allowed)

All values:

lowercase

predefined

no free-text

no creativity

ğŸš« INVALID:

"domain": "electronics"
"domain": ["tech", "gadgets"]
"domain": []

5. Standardization & normalization rules
5.1 Domain source (LOCKED)

Domains come from predefined lists only:

Product Domains

Service Domains

Mutual Categories (mapped separately, see primary_mutual_category)

âš ï¸ The model must NEVER invent new domains

5.2 Multi-domain rules

Use multiple domains only if the query genuinely spans multiple spaces

Do NOT over-assign

Prefer specific over generic

Example:

â€œrepair laptop screenâ€

"domain": ["technology & electronics"]


Not:

["technology", "services", "hardware"]

6. What the model MUST do

Select the closest matching predefined domain

Use semantic meaning, not keyword frequency

Prefer market-understood categories

Assign minimum necessary domains

7. What the model MUST NEVER do

ğŸš« Invent new domain names
ğŸš« Use sub-domain terms as domains
ğŸš« Encode attributes in domain
ğŸš« Omit domain
ğŸš« Change schema shape
ğŸš« Over-generalize when specificity exists

8. INTERNAL CoT QUESTIONS (DATA GENERATION ONLY)

âš ï¸ INTERNAL USE ONLY â€” NEVER OUTPUT

Q1: Is the request about a tangible item?
    â†’ Use Product Domain list

Q2: Is the request about work, skill, or service?
    â†’ Use Service Domain list

Q3: Is the request about a human relationship or shared activity?
    â†’ Use Mutual Categories (domain stays empty or generic)

Q4: What is the CLOSEST predefined domain?
    (Do not invent new labels)

Q5: Does the request truly span two domains?
    â†’ If YES, include both
    â†’ If NO, choose the dominant one

Q6: If no reasonable domain fits â†’ INVALID
Q12: What TYPE of entity is being sought?
    THING (tangible item) â†’ Product domains (21 options)
    WORK/EXPERTISE (service) â†’ Service domains (18 options)
    PERSON (peer for shared activity) â†’ Mutual categories (25 options)

Q13: Which EXISTING domain/category is CLOSEST match?
    Use semantic similarity to map to fixed list
    NEVER create new domains - always map to existing 64
    For unseen entities, find closest semantic match

Q14: VALIDATE domain/category assignment:
    For PRODUCT/SERVICE: domain = valid, primary_mutual_category = null
    For MUTUAL: domain = null, primary_mutual_category = valid

9. Positive examples (TRUE POSITIVES)
Product
Query	domain
â€œbuy iphone 14â€	["technology & electronics"]
â€œselling used bikeâ€	["automotive & vehicles"]
Service
Query	domain
â€œneed math tutorâ€	["education & training"]
â€œplumber neededâ€	["construction & trades"]
Mutual
Query	domain
â€œlooking for a roommateâ€	["real estate & property"]
â€œgym buddy neededâ€	["sports & outdoors"]
10. Negative examples
âŒ Hard Negatives
Case	Why invalid
new domain invented	schema violation
empty domain	mandatory missing
attribute used as domain	misuse
âŒ False Positives
Query	Wrong domain	Correct
â€œbuy gaming laptopâ€	["gaming"]	["technology & electronics"]
â€œneed yoga instructorâ€	["yoga"]	["alternative & holistic health"]
âŒ False Negatives
Query	Missed domain
â€œneed car repairâ€	["automotive & vehicles"]
â€œselling office deskâ€	["home & furniture"]
11. Edge cases & ambiguity handling
Case 1: Generic phrasing

â€œneed help with something onlineâ€

â†’ INVALID (no clear domain)

Case 2: Platform vs domain

â€œselling on amazonâ€

Domain is NOT â€œe-commerceâ€
Choose based on item/service, not platform.

Case 3: Multiple domain overlap

â€œfitness app developmentâ€

"domain": ["technology & electronics", "fitness & wellness"]


Only if BOTH are essential.

12. Validation checks

A sample is INVALID if:

domain is empty

domain not in predefined list

domain invented or free-text

domain contradicts intent

excessive domains without semantic justification

FINAL LOCK (DO NOT CHANGE)

domain answers WHAT space the problem belongs to.
It never encodes attributes, direction, or constraints.
It is chosen from a fixed universe and nothing else.

### PRIMARY_MUTUAL_CATEGORY
FIELD SPECIFICATION â€” primary_mutual_category
1. Definition

primary_mutual_category identifies the core human relationship or shared activity type in a mutual intent.

It answers:

â€œWHAT kind of human-to-human connection is being sought?â€

This field exists ONLY to specialize mutual intent beyond domain-level abstraction.

2. When this field MUST be populated

ONLY IF

intent = "mutual"


Mandatory for all mutual queries

If intent = mutual and this field is empty â†’ INVALID sample

3. When this field MUST be empty

ALWAYS EMPTY if:

intent = product OR service


If populated for product/service â†’ INVALID

4. Allowed structure & data type
"primary_mutual_category": ["<category_string>"]


Type: array of strings

Minimum length: 1

Maximum length: 1 (single primary category only)

Lowercase only

Predefined list only

No free-text

No creativity

ğŸš« INVALID:

"primary_mutual_category": []
"primary_mutual_category": ["friendship", "travel"]
"primary_mutual_category": "roommate"

5. Standardization & normalization rules
5.1 Source of truth (LOCKED)

Categories must come from the predefined Mutual Category List (25 items you locked earlier):

housing
roommates
fitness
sports
partners
travel
adventure
learning
study
professional
career
social
friendship
dating
relationships
parenting
family
hobbies
interests
pets
animals
support
caregiving
community
volunteering


âš ï¸ The model must NEVER invent a new category.

5.2 One-category rule (CRITICAL)

Choose ONE category only

Pick the dominant shared purpose

Secondary interests belong in attributes, NOT here

6. What the model MUST do

Populate this field only for mutual intent

Select exactly one category

Use semantic meaning, not keyword matching

Prefer human-understood relationship types

Choose the strongest signal

7. What the model MUST NEVER do

ğŸš« Populate for product or service
ğŸš« Leave empty for mutual
ğŸš« Output multiple categories
ğŸš« Encode attributes or preferences
ğŸš« Invent categories
ğŸš« Use domain names as categories

8. INTERNAL CoT QUESTIONS (DATA GENERATION ONLY)

âš ï¸ INTERNAL USE ONLY â€” NEVER OUTPUT

Q1: Is the user seeking a HUMAN connection?
    â†’ If NO â†’ this field must be empty

Q2: Is participation SYMMETRIC?
    â†’ If NO â†’ not mutual

Q3: What is the PRIMARY shared purpose?
    (Ignore attributes, preferences, constraints)

Q4: Which ONE mutual category best describes it?
    â†’ Choose ONLY ONE

Q5: If none fit â†’ INVALID

9. Positive examples (TRUE POSITIVES)
Query	primary_mutual_category
â€œneed a roommate in indiranagarâ€	["roommates"]
â€œlooking for gym buddyâ€	["fitness"]
â€œwant a travel partner to goaâ€	["travel"]
â€œseeking cofounder for startupâ€	["professional"]
â€œlooking for hiking groupâ€	["adventure"]
10. Negative examples
âŒ Hard Negatives
Case	Why invalid
empty for mutual	mandatory missing
more than one category	schema violation
category invented	not predefined
âŒ False Positives
Query	Wrong category	Correct
â€œlooking for gym buddyâ€	["sports"]	["fitness"]
â€œneed cofounderâ€	["career"]	["professional"]
âŒ False Negatives
Query	Missed category
â€œsomeone to share flat withâ€	["roommates"]
â€œanyone into book clubs?â€	["hobbies"]
11. Edge cases & ambiguity handling
Case 1: Multiple activities mentioned

â€œlooking for friend to travel and trekâ€

Choose dominant motivation: IF BOTH DOMAINS ARE LOOKING THEN IT CAN BE BOTH 

["travel"]


Secondary interests handled as attributes later.

Case 2: Relationship implied indirectly

â€œneed someone to split rent withâ€

â†’ ["roommates"]

Case 3: Professional mutual vs service

â€œlooking for business partnerâ€

No payment

Equal stake
â†’ mutual + ["professional"]

12. Validation checks

A sample is INVALID if:

intent â‰  mutual AND category populated

intent = mutual AND category missing

category not in predefined list

more than one category

category used to encode attributes

FINAL LOCK (DO NOT CHANGE)

primary_mutual_category defines the HUMAN RELATIONSHIP TYPE.
One mutual query â†’ one dominant category â†’ nothing more.

### SECTION E: ITEMS & ITEM_EXCLUSIONS
ITEMS, ATTRIBUTES & EXCLUSIONS

(CRITICAL CORE â€” DO NOT WEAKEN)

Covers Fields

items

item_exclusions

(attributes live inside items using axis â†’ min/max/range)

This section is the foundation of matching.
Mistakes here break SQL filtering and determinism.

FIELD: items
1. Definition

items represents WHAT thing is being interacted with,
regardless of intent, ownership, state, or action.

It is intent-agnostic.

product â†’ thing

service â†’ service-type
 
"""irrespective of the intent if product or service is present in the query then add "

2. When this field MUST be populated

Populate items IF AND ONLY IF the query mentions:

a tangible object

a service / work type

a subject being exchanged, discussed, or interacted with  and it must be product or service cannonicalize to market standard in language level not ontology level using polysemy

Examples:

â€œiphoneâ€, â€œlaptopâ€, â€œbikeâ€

â€œplumberâ€, â€œmath tutorâ€, â€œyoga instructorâ€

â€œlanguage exchangeâ€, â€œskill swapâ€, â€œlost phoneâ€

3. When this field MUST be empty

Leave items = [] ONLY when:

the query is purely social/emotional with no subject

the query is meta (â€œanyone here?â€, â€œjust browsingâ€)

4. Allowed structure & data type
"items": [
  {
    "type": "<canonical market noun>",
    "categorical": {
      "<key>": "<value>"
    },
    "min": {
      "<axis>": [{ "type": "", "value": <num>, "unit": "" }]
    },
    "max": {
      "<axis>": [{ "type": "", "value": <num>, "unit": "" }]
    },
    "range": {
      "<axis>": [{ "type": "", "min": <num>, "max": <num>, "unit": "" }]
    }
  }
]

FIELD PURPOSES:
- type: What it is (canonical market noun)
- categorical: Non-numeric attributes (condition, fuel, color, brand, etc.)
- min: Minimum constraint (numeric attributes with axis)
- max: Maximum constraint (numeric attributes with axis)
- range: Exact value (min=max) OR range (minâ‰ max)

Rules:

type â†’ REQUIRED

categorical â†’ OPTIONAL (only if non-numeric attributes stated)

min/max/range â†’ OPTIONAL (only if numeric constraints stated)

Multiple items allowed

Each item is independent

5. Standardization & Normalization Rules
5.1 Type Standardization (LANGUAGE-LEVEL)

âœ”ï¸ Allowed (LLM responsibility):

Query phrase	items.type
iphone	smartphone
mobile phone	smartphone
pipe leakage	plumbing
need a plumber	plumbing
math tutor	tutoring
yoga instructor	yoga
lost phone	smartphone

âŒ Forbidden:

Creating new ontology nodes

Encoding state/action into type

Examples âŒ:

lost_item

skill_exchange

language_exchange

5.1.1 Compound Type Decomposition (MANDATORY)

When a query contains a compound phrase (modifier + noun), ALWAYS decompose:

LINGUISTIC RULE (HEAD-FINAL):
In English, compound nouns are HEAD-FINAL. The LAST/RIGHTMOST noun is the "head" (what it IS).
Preceding words are MODIFIERS (properties/attributes of it).

DECOMPOSITION PROCESS:
1. Identify the RIGHTMOST/HEAD noun â†’ this becomes `type`
2. Identify MODIFYING words (adjectives, qualifying nouns) â†’ these become `categorical` attributes
3. Apply market noun standardization to `type` if needed

| Query Phrase | type | categorical |
|--------------|------|-------------|
| "golden retriever puppy" | puppy | categorical: { breed: "golden retriever" } |
| "Persian cat" | cat | categorical: { breed: "persian" } |
| "Apple iPhone" | smartphone | categorical: { brand: "apple" } |
| "used Dell laptop" | laptop | categorical: { brand: "dell", condition: "used" } |
| "red BMW sedan" | sedan | categorical: { brand: "bmw", color: "red" } |
| "3 month old labrador" | dog | categorical: { breed: "labrador" }, range: { time: [{ type: "age", min: 3, max: 3, unit: "months" }] } |
| "second hand Toyota car" | car | categorical: { brand: "toyota", condition: "used" } |

CoT Decision Gate (INTERNAL ONLY):
Q1: Is this a compound phrase (multiple words describing one thing)?
Q2: What is the RIGHTMOST noun? â†’ This is the HEAD/type
Q3: What words MODIFY the head? â†’ These go to categorical
Q4: Does the head need market noun standardization? (e.g., "iPhone" alone â†’ "smartphone")

âŒ NEVER put the full compound in type:
- âŒ type: "golden retriever puppy"
- âŒ type: "Apple iPhone"
- âŒ type: "used Dell laptop"

âœ”ï¸ ALWAYS decompose:
- âœ”ï¸ type: "puppy", categorical: { breed: "golden retriever" }
- âœ”ï¸ type: "smartphone", categorical: { brand: "apple" }
- âœ”ï¸ type: "laptop", categorical: { brand: "dell", condition: "used" }

----------------------------------------------------------------
5.1.2 LIFE-STAGE NOUNS (ANIMAL/HUMAN AGE TERMS)
----------------------------------------------------------------

Life-stage words ARE the type (they encode both species + age semantically):

| Term | Type | Implicit Age | DO NOT add age constraint |
|------|------|--------------|---------------------------|
| "puppy" | puppy | <1 year | Puppy already implies young dog |
| "kitten" | kitten | <1 year | Kitten already implies young cat |
| "calf" | calf | <1 year | Calf already implies young cow |
| "foal" | foal | <1 year | Foal already implies young horse |
| "lamb" | lamb | <1 year | Lamb already implies young sheep |
| "infant" | infant | 0-1 year | Type encodes age |
| "toddler" | toddler | 1-3 years | Type encodes age |

RULE: Life-stage nouns are VALID types. Do NOT decompose further.
- âœ… "golden retriever puppy" â†’ type: "puppy", breed: "golden retriever"
- âœ… "labrador puppy" â†’ type: "puppy", breed: "labrador"
- âœ… "puppy" â†’ type: "puppy" (no breed specified)
- âŒ "puppy" â†’ type: "dog", age: "young" (WRONG - redundant)

WHEN AGE IS EXPLICITLY STATED, ADD IT:
- "3 month old puppy" â†’ type: "puppy", range: { time: [{ type: "age", min: 3, max: 3, unit: "months" }] }
- "6 week kitten" â†’ type: "kitten", range: { time: [{ type: "age", min: 6, max: 6, unit: "weeks" }] }

BREED-AS-STANDALONE RESOLUTION:
When only breed name is given, infer species type:
- "labrador" â†’ type: "dog", breed: "labrador" (labrador is a dog breed)
- "persian" â†’ type: "cat", breed: "persian" (persian is a cat breed)
- "holstein" â†’ type: "cow", breed: "holstein" (holstein is a cow breed)

----------------------------------------------------------------
5.1.3 CATEGORICAL KEY SELECTION (DOMAIN-SPECIFIC)
----------------------------------------------------------------

Different domains use different attribute keys for sub-classification:

| Domain | Key | Usage |
|--------|-----|-------|
| Pets & Animals | breed | Animal variety (labrador, persian, beagle) |
| Automotive & Vehicles | brand + model | Manufacturer + product line (toyota camry) |
| Technology & Electronics | brand + model | Manufacturer + product line (apple iphone) |
| Fashion & Apparel | brand | Manufacturer (nike, adidas) |
| Real Estate & Property | property_type | Type of property (apartment, villa) |

RULE: Use domain-appropriate keys, not invented ones.
- âœ… categorical: { breed: "labrador" } (for pets)
- âœ… categorical: { brand: "toyota", model: "camry" } (for vehicles)
- âŒ categorical: { type: "labrador" } (wrong key for pets)
- âŒ categorical: { variety: "golden retriever" } (invented key)

5.2 Polysemy Handling (MANDATORY)

CoT Decision Gate (USED FOR GENERATION, NOT OUTPUT)

Q1: Is the phrase referring to an OBJECT, SERVICE, or SUBJECT?
Q2: Is the surface word ambiguous?
Q3: Does the surrounding language clarify the base noun?
â†’ Map to the BASE MARKET NOUN


âœ”ï¸ "leakage in pipes" â†’ plumbing
âœ”ï¸ "cracked phone" â†’ smartphone
âœ”ï¸ "math teacher" â†’ tutoring

----------------------------------------------------------------
5.2.1 POLYSEMY RESOLUTION TABLE (COMMON AMBIGUOUS WORDS)
----------------------------------------------------------------

Use DOMAIN + CONTEXT to resolve ambiguous words deterministically:

| Word | Domain/Context | Resolves To | Type |
|------|----------------|-------------|------|
| "notebook" | Technology & Electronics | laptop | laptop |
| "notebook" | Office & Stationery | paper notebook | notebook |
| "tablet" | Technology & Electronics | tablet computer | tablet |
| "tablet" | Healthcare & Wellness | medicine tablet | medication |
| "mouse" | Technology & Electronics | computer mouse | mouse |
| "mouse" | Pets & Animals | rodent pet | mouse |
| "coach" | Transportation & Logistics | bus/vehicle | coach |
| "coach" | Education & Training | trainer/mentor | coaching |
| "driver" | Technology & Electronics | software driver | driver |
| "driver" | Transportation & Logistics | vehicle operator | driver |
| "plant" | Agriculture & Farming | vegetation | plant |
| "plant" | Manufacturing & Production | factory | factory |
| "watch" | Fashion & Apparel | wristwatch | watch |
| "watch" | Entertainment & Events | to view | (verb - ignore) |
| "cell" | Technology & Electronics | mobile phone | smartphone |
| "cell" | Healthcare & Wellness | biological cell | (not a product) |
| "mac" | Technology & Electronics | Apple computer | laptop |
| "mac" | Food & Beverage | macaroni | (context-dependent) |

RESOLUTION PRIORITY:
1. Explicit domain keyword in query â†’ use that domain
2. Co-occurring context words â†’ infer domain
3. Default to most common market meaning

EXAMPLES:
- "need a notebook for coding" â†’ domain: Technology & Electronics â†’ type: "laptop"
- "need a notebook for notes" â†’ domain: Office & Stationery â†’ type: "notebook"
- "mouse for gaming" â†’ domain: Technology & Electronics â†’ type: "mouse" (computer)
- "pet mouse" â†’ domain: Pets & Animals â†’ type: "mouse" (animal)

6. Attributes (INSIDE items)

Attributes are constraints on items
They MUST attach to one of the fixed axes.

Fixed Attribute Axes (LOCKED)
identity
capacity
performance
quality
quantity
time
space
cost
mode
skill


No new axes. Ever.

7. Attribute Extraction â€” CoT (CONTROL QUESTIONS)

These questions guide extraction but NEVER appear in output.

Q15: Does the query specify a PROPERTY of the item?
     (storage, price, experience, rating, size, speed)

Q16: Is the value quantitative, categorical, boolean, or temporal?

Q17: Is a constraint implied?
     - under / below â†’ max
     - over / at least â†’ min
     - between â†’ range
     - exact mention â†’ min=max

Q18: Is the unit explicitly stated?
     - If yes â†’ extract
     - If no â†’ use 'local' ONLY for currency
**Covers Fields**: other_party_preferences, self_attributes



8. Min / Max / Range Rules (STRICT)

Exact DOES NOT EXIST

Exact = min == max

Valid
"min": {
  "capacity": [
    { "type": "storage", "value": 1024, "unit": "gb" }
  ]
}

INVALID âŒ
"capacity": [{ "type": "storage", "value": 1024 }]

9. Numeric Normalization Rules
Allowed (LOSSLESS ONLY)
Input	Stored
512mb	0.5 gb
1024mb	1 gb
1tb	1024 gb
3 years	36 months
Forbidden âŒ

HDD â†” SSD

USD â†’ INR

degree equivalence

assumed units

10. Country-Specific vs Universal
Universal (convertible)

time

distance

storage

weight

area

Country-specific (preserve)

currency

education labels

legal terms

If currency not stated â†’ unit = local

11. What the model MUST do

âœ”ï¸ Extract only stated facts
âœ”ï¸ Normalize numeric values losslessly
âœ”ï¸ Standardize type to market noun
âœ”ï¸ Attach attributes ONLY via axes
âœ”ï¸ Use min/max/range consistently
âœ”ï¸ Keep schema shape identical always

12. What the model MUST NEVER do

ğŸš« Invent attributes
ğŸš« Encode action or state in type
ğŸš« Create new axes
ğŸš« Guess missing values
ğŸš« Perform ontology reasoning
ğŸš« Convert country-specific units

13. Positive Examples (TRUE POSITIVES)
Example 1

â€œlooking to buy a laptop with 1tb ssd and 24gb ramâ€

"items": [
  {
    "type": "laptop",
    "min": {
      "capacity": [
        { "type": "storage", "value": 1024, "unit": "gb" },
        { "type": "memory", "value": 24, "unit": "gb" }
      ]
    }
  }
]

Example 2

"need someonewho fix  pipe leakage"

"items": [
  { "type": "plumbing" }
]

Example 3 (COMPOUND DECOMPOSITION)

"selling my golden retriever puppy, 3 months old"

"items": [
  {
    "type": "puppy",
    "categorical": {
      "breed": "golden retriever",
      "age": "3 months"
    }
  }
]

Example 4 (COMPOUND DECOMPOSITION)

"looking for a used Apple MacBook Pro"

"items": [
  {
    "type": "laptop",
    "categorical": {
      "brand": "apple",
      "model": "macbook pro",
      "condition": "used"
    }
  }
]

14. Negative Examples
Hard Negatives âŒ

lost_item

skill_exchange

language_exchange

pipe_leak_service

golden_retriever_puppy (NEVER put compound in type)

apple_iphone (NEVER put brand+product as type)

False Positive âŒ

Extracting attributes not mentioned

Putting full compound phrase in type instead of decomposing

False Negative âŒ

Failing to extract smartphone in "lost phone"

Failing to decompose "Persian cat" into type: "cat", categorical: { breed: "persian" }

15. Edge Cases & Ambiguity Handling

âœ”ï¸ If unsure â†’ choose base noun
âœ”ï¸ If multiple items â†’ extract all
âœ”ï¸ If attributes vague â†’ skip attributes
âœ”ï¸ Intent NEVER changes item extraction

16. Validation Checks (STAGE 4)

Reject if:

axis appears outside min/max/range

type encodes action/state

non-market word used

schema deviates

numeric unit missing

type contains compound phrase (should be decomposed into type + categorical)

FINAL LOCK (DO NOT CHANGE)

Items capture WHAT, not HOW, not WHY, not STATE.

Attributes capture constraints only when explicit.

All intelligence beyond language normalization happens AFTER the model.


### OTHER_PREFERENCES

Definition

other_party_preferences represents constraints and expectations about the OTHER PERSON,
not about the product/service itself and not about the user.

It answers:

â€œWhat must the other person be / have / do?â€

This field is person-centric only.

2ï¸âƒ£ When this field MUST be populated

Populate only if the query explicitly states requirements about the other person, such as:

Skills or experience of the person

Personal traits (gender, age, profession)

Language, location, background

Lifestyle or habits (non-smoker, vegetarian, etc.)

Examples (MUST populate)

â€œNeed a software developer who speaks Kannadaâ€

â€œLooking for a roommate female, age 25â€“30â€

â€œNeed a tutor with 3 years of experienceâ€

â€œDriver from Karnatakaâ€

â€œNon-smoker preferredâ€

3ï¸âƒ£ When this field MUST be empty

MUST be {} if no person-specific constraints exist.

Examples (MUST be empty)

â€œLooking for an iPhone under 30kâ€

â€œLaptop repair service neededâ€

â€œSelling my Royal Enfieldâ€

â€œNeed plumbing service urgentlyâ€ (unless plumber traits are specified)

ğŸš« Do NOT move product/service attributes here.

COT:
```
Q20: What preferences about the OTHER PERSON are mentioned?
    For buy: About SELLER (language, verified, no agents)
    For sell: About BUYER (serious, payment method)
    For seek: About PROVIDER (experience, rating, certified)
    For provide: About CUSTOMER (budget, timeline)
    For connect: About PARTNER (age, gender, diet, smoking)
    â†’ Extract to other_party_preferences

Q21: What SELF attributes are mentioned (user describing themselves)?
    "I am female", "I'm a software engineer", "non-smoker here"
    â†’ Extract to self_attributes
    Note: Mainly used for MUTUAL and SERVICE provide

Q22: Apply YES/NO FLAGS for person traits:
    Positive: "verified seller" â†’ verified: "yes"
    Negative: "no agents" â†’ agent: "no"
    Semantic expansion:
    - "no bad habits" â†’ smoking: "no", drinking: "no"
    - "clean lifestyle" â†’ smoking: "no", drinking: "no"
    - "vegetarian" â†’ diet: "vegetarian"
```

4ï¸âƒ£ Allowed structure & data types (LOCKED)
"other_party_preferences": {
  "identity": [ { "type": "", "value": "" } ],
  "lifestyle": [ { "type": "", "value": "" } ],
  "habits": {
    "<flag_name>": "yes | no"
  },
  "min": {
    "<axis>": [
      { "type": "", "value": <number>, "unit": "" }
    ]
  },
  "max": {
    "<axis>": [
      { "type": "", "value": <number>, "unit": "" }
    ]
  },
  "range": {
    "<axis>": [
      { "type": "", "min": <number>, "max": <number>, "unit": "" }
    ]
  }
}


Empty arrays/objects are VALID.

5ï¸âƒ£ Standardization & Normalization Rules
Identity

Single-word, market terms

Lowercase

No assumptions

Examples:

female, male
software engineer
student
teacher

Numeric attributes

Must use min / max / range

Exact = min = max

Normalize units (lossless only)

Examples:

3 years â†’ 36 months
25 years old â†’ range.age = [25,25]

Location (person-based)

Extract ONLY if referring to the other person

Example:

â€œDeveloper from Karnatakaâ€

"identity": [
  { "type": "location", "value": "karnataka" }
]

Habits (FLAGS ONLY)

Binary ONLY.

Examples:

smoking: "no"
drinking: "no"
pets: "yes"


ğŸš« No exclusions array
ğŸš« No partial values

6ï¸âƒ£ What the model MUST do

âœ… Extract only person-related facts
âœ… Use identity / lifestyle / habits correctly
âœ… Normalize numeric values
âœ… Use market-standard words
âœ… Preserve country-specific semantics
âœ… Leave empty if not stated

7ï¸âƒ£ What the model MUST NEVER do

ğŸš« Move product/service attributes here
ğŸš« Infer skills, age, gender
ğŸš« Convert currency, language, education
ğŸš« Use exclusions instead of flags
ğŸš« Invent new keys
ğŸš« Use free-text blobs

8ï¸âƒ£ Positive Examples (TRUE POSITIVES)
Example 1

Query

â€œNeed a software developer who speaks Kannada and has 3 years of React experience.â€

"other_party_preferences": {
  "identity": [
    { "type": "language", "value": "kannada" }
  ],
  "min": {
    "time": [
      { "type": "experience", "value": 36, "unit": "month" }
    ]
  }
}


(React skill â†’ goes to items, NOT here)

Example 2

Query

â€œLooking for a roommate, female, 25â€“30, non-smoker.â€

"other_party_preferences": {
  "identity": [
    { "type": "gender", "value": "female" }
  ],
  "range": {
    "time": [
      { "type": "age", "min": 25, "max": 30, "unit": "year" }
    ]
  },
  "habits": {
    "smoking": "no"
  }
}

9ï¸âƒ£ Negative Examples
âŒ Hard Negative
"other_party_preferences": {
  "min": {
    "cost": [ { "type": "price", "value": 30000 } ]
  }
}


âŒ Price belongs to item/service, not person.

âŒ False Positive

Extracting preferences when none stated.

Query:

â€œNeed laptop repairâ€

âŒ Any person preferences extracted â†’ WRONG

âŒ False Negative

Ignoring explicit person constraint.

Query:

â€œNeed a driver who speaks Tamilâ€

âŒ Leaving preferences empty â†’ WRONG

ğŸ”Ÿ Edge Cases & Ambiguity Handling
â€œPreferably femaleâ€

â†’ Extract identity.gender = female

â€œExperienced developerâ€

â†’ ONLY extract if numeric given
âŒ â€œexperiencedâ€ alone â†’ ignore

â€œGood person / reliableâ€

â†’ Ignore unless measurable (rating, years)

11ï¸âƒ£ Validation Checks (MANDATORY)

Reject if:

Any product attribute appears here

Any inferred preference exists

Any habit not binary

Any numeric without min/max/range

Any invented key appears

ğŸ”’ FINAL INVARIANT (SAVE THIS)
other_party_preferences =
ONLY person-related requirements
NO product/service traits
NO inference
NO creativity
FLAGS instead of exclusions

### self_attributes
FIELD SPECIFICATION: self_attributes
1ï¸âƒ£ Definition

self_attributes represents facts the USER explicitly states about THEMSELVES.

It answers:

â€œWhat am I?â€

This field is self-descriptive only, never aspirational and never inferred.

2ï¸âƒ£ When this field MUST be populated

Populate only when the user explicitly states information about themselves, such as:

Skills they have

Their profession / role

Age, gender (only if stated)

Languages they speak

Habits or lifestyle choices

Location they belong to (if stated as identity, not target)

Examples (MUST populate)

â€œIâ€™m a software developer with 5 years experienceâ€

â€œIâ€™m a non-smokerâ€

â€œ25-year-old femaleâ€

â€œI run every morningâ€

â€œIâ€™m from Karnatakaâ€

3ï¸âƒ£ When this field MUST be empty

MUST be {} when:

User does not describe themselves

Query is purely about buying/selling/seeking

The description is aspirational or implied

Examples (MUST be empty)

â€œNeed a plumber urgentlyâ€

â€œLooking for a roommateâ€

â€œSelling my laptopâ€

â€œNeed a developerâ€

4ï¸âƒ£ Allowed structure & data types (LOCKED)
"self_attributes": {
  "identity": [
    { "type": "", "value": "" }
  ],
  "lifestyle": [
    { "type": "", "value": "" }
  ],
  "habits": {
    "<flag_name>": "yes | no"
  },
  "min": {
    "<axis>": [
      { "type": "", "value": <number>, "unit": "" }
    ]
  },
  "max": {
    "<axis>": [
      { "type": "", "value": <number>, "unit": "" }
    ]
  },
  "range": {
    "<axis>": [
      { "type": "", "min": <number>, "max": <number>, "unit": "" }
    ]
  }
}


Empty arrays / objects are VALID and EXPECTED.

```
Q20: What preferences about the OTHER PERSON are mentioned?
    For buy: About SELLER (language, verified, no agents)
    For sell: About BUYER (serious, payment method)
    For seek: About PROVIDER (experience, rating, certified)
    For provide: About CUSTOMER (budget, timeline)
    For connect: About PARTNER (age, gender, diet, smoking)
    â†’ Extract to other_party_preferences

Q21: What SELF attributes are mentioned (user describing themselves)?
    "I am female", "I'm a software engineer", "non-smoker here"
    â†’ Extract to self_attributes
    Note: Mainly used for MUTUAL and SERVICE provide

Q22: Apply YES/NO FLAGS for person traits:
    Positive: "verified seller" â†’ verified: "yes"
    Negative: "no agents" â†’ agent: "no"
    Semantic expansion:
    - "no bad habits" â†’ smoking: "no", drinking: "no"
    - "clean lifestyle" â†’ smoking: "no", drinking: "no"
    - "vegetarian" â†’ diet: "vegetarian"
```

5ï¸âƒ£ Standardization & Normalization Rules
Identity

Single word

Lowercase

Market-recognized term

Examples:

software developer
student
designer
female

Numeric attributes

Must follow min / max / range

Exact = min = max

Normalize losslessly

Examples:

5 years â†’ 60 months
age 25 â†’ range.age [25,25]

Habits (FLAGS ONLY â€” critical)

Binary, explicit, deterministic.

Examples:

smoking: "no"
drinking: "no"
pets: "yes"


ğŸš« Do NOT use exclusions
ğŸš« Do NOT infer

6ï¸âƒ£ What the model MUST do

âœ… Extract only what user explicitly says about themselves
âœ… Use flags for habits
âœ… Normalize numeric values
âœ… Use standard market words
âœ… Leave empty if unstated

7ï¸âƒ£ What the model MUST NEVER do

ğŸš« Infer self attributes from intent
ğŸš« Assume profession or skill
ğŸš« Copy other_party_preferences here
ğŸš« Convert education/country terms
ğŸš« Use exclusions instead of flags
ğŸš« Invent descriptors

8ï¸âƒ£ Positive Examples (TRUE POSITIVES)
Example 1

Query

â€œIâ€™m a software engineer with 4 years experience, non-smoker.â€

"self_attributes": {
  "identity": [
    { "type": "profession", "value": "software engineer" }
  ],
  "min": {
    "time": [
      { "type": "experience", "value": 48, "unit": "month" }
    ]
  },
  "habits": {
    "smoking": "no"
  }
}

Example 2

Query

â€œ25-year-old female, based in Bangalore.â€

"self_attributes": {
  "identity": [
    { "type": "gender", "value": "female" },
    { "type": "location", "value": "bangalore" }
  ],
  "range": {
    "time": [
      { "type": "age", "min": 25, "max": 25, "unit": "year" }
    ]
  }
}

9ï¸âƒ£ Negative Examples
âŒ Hard Negative
"self_attributes": {
  "min": {
    "cost": [ { "type": "price", "value": 20000 } ]
  }
}


âŒ Cost belongs to item/service, never the person.

âŒ False Positive

Extracting attributes when user didnâ€™t self-identify.

Query:

â€œNeed a roommateâ€

âŒ Any self_attributes â†’ WRONG

âŒ False Negative

Failing to extract explicit self description.

Query:

â€œI donâ€™t smoke and Iâ€™m vegetarianâ€

âŒ Empty self_attributes â†’ WRONG

ğŸ”Ÿ Edge Cases & Ambiguity Handling
â€œI prefer not to smokeâ€

â†’ âŒ This is preference, not self
â†’ Goes to other_party_preferences.habits

â€œI usually work weekendsâ€

â†’ Only extract if clearly about SELF availability
Otherwise ignore.

â€œExperienced professionalâ€

â†’ âŒ Ignore unless numeric provided

11ï¸âƒ£ Validation Checks (MANDATORY)

Reject if:

Any product or service attribute appears

Any inferred self info exists

Habits are not binary

Numeric values lack min/max/range

Non-deterministic language appears

ğŸ”’ FINAL LOCK FOR self_attributes
self_attributes =
FACTS ABOUT USER ONLY
NO ASSUMPTIONS
NO ASPIRATION
NO PRODUCT LOGIC
FLAGS > EXCLUSIONS

### LOCATION

FIELD SPECIFICATION: LOCATION HANDLING

Covers Fields

target_location

location_match_mode

location_exclusions

These must always be reasoned together. No partial extraction.

1ï¸âƒ£ Definition
target_location

Represents where matching should happen, not where the user currently is.

location_match_mode

Defines HOW location should be interpreted, never inferred.

location_exclusions

Defines places explicitly excluded by the user.

Location is query-driven only.
App/device/user profile location is injected later by the system â€” never by the model.

2ï¸âƒ£ When these fields MUST be populated
location_match_mode

âœ… ALWAYS populated
It is never optional.

target_location

Populate ONLY if:

A location is explicitly mentioned

OR movement / destination is mentioned

OR route is mentioned

location_exclusions

Populate ONLY if:

User explicitly excludes a place

3ï¸âƒ£ When these fields MUST be empty
target_location MUST be {} when:

No explicit location is mentioned

Location is implied but not stated

location_exclusions MUST be [] when:

No exclusions are stated

4ï¸âƒ£ Allowed Values & Structure (LOCKED)
location_match_mode (ENUM â€“ FROZEN)
near_me | explicit | target_only | route | global

target_location shapes

Single location

"target_location": {
  "name": "bangalore"
}


Route

"target_location": {
  "origin": "delhi",
  "destination": "mumbai"
}


Global / Remote

"target_location": {}

location_exclusions
"location_exclusions": ["chennai", "noida"]


Lowercase

Plain names

No geo-coding

No hierarchy

5ï¸âƒ£ Standardization & Normalization Rules

Preserve user-stated strings

Lowercase only

Do NOT infer country/state

Do NOT expand abbreviations unless obvious (blr â†’ bangalore âŒ)

Do NOT geo-resolve

6ï¸âƒ£ LOCATION COT (FOR DATA GENERATION & FINE-TUNING ONLY)

This logic is allowed internally, but must NEVER appear in output.

Q6: Is a location explicitly mentioned?
    YES â†’ extract
    NO â†’ target_location = {}

Q7: Is there movement or relocation?
    "moving to", "relocating to" â†’ target_only
    "travel from X to Y" â†’ route

Q8: Are two locations mentioned?
    YES â†’ origin + destination
    NO â†’ single location

Q9: Is it remote / online / anywhere?
    YES â†’ global

Q10: Are exclusions mentioned?
    YES â†’ location_exclusions[]
    NO â†’ []

Q11: Assign mode:
    Default â†’ near_me

7ï¸âƒ£ What the model MUST do

âœ… Always assign location_match_mode
âœ… Extract location ONLY if explicitly stated
âœ… Distinguish static vs movement vs route
âœ… Preserve exclusions exactly
âœ… Leave target_location empty when unstated

8ï¸âƒ£ What the model MUST NEVER do

ğŸš« Infer current location
ğŸš« Inject app/device location
ğŸš« Guess geography
ğŸš« Convert city â†’ state â†’ country
ğŸš« Add exclusions implicitly
ğŸš« Create new modes

9ï¸âƒ£ Positive Examples (TRUE POSITIVES)
Example 1 â€” No location

Query

â€œNeed a yoga instructorâ€

"target_location": {},
"location_match_mode": "near_me",
"location_exclusions": []

Example 2 â€” Explicit local

Query

â€œLooking for a plumber in Andheri Westâ€

"target_location": { "name": "andheri west" },
"location_match_mode": "explicit",
"location_exclusions": []

Example 3 â€” Relocation

Query

â€œMoving to Pune, need a roommateâ€

"target_location": { "name": "pune" },
"location_match_mode": "target_only",
"location_exclusions": []

Example 4 â€” Travel route

Query

â€œLooking for a travel buddy from Bangalore to Goaâ€

"target_location": {
  "origin": "bangalore",
  "destination": "goa"
},
"location_match_mode": "route",
"location_exclusions": []

Example 5 â€” Remote / global

Query

â€œLooking for a remote frontend developerâ€

"target_location": {},
"location_match_mode": "global",
"location_exclusions": []

Example 6 â€” Exclusion

Query

â€œNeed a flatmate in Bangalore, not Whitefieldâ€

"target_location": { "name": "bangalore" },
"location_match_mode": "explicit",
"location_exclusions": ["whitefield"]

ğŸ”Ÿ Negative Examples
âŒ Hard Negative

Inferring location.

Query:

â€œNeed a developerâ€

âŒ Extracting any location â†’ INVALID

âŒ False Positive
"location_match_mode": "explicit"


when no location is mentioned â†’ INVALID

âŒ False Negative

Failing to extract exclusion.

Query:

â€œAnywhere except Delhiâ€

âŒ location_exclusions empty â†’ INVALID

11ï¸âƒ£ Edge Cases & Ambiguity Handling
â€œnear metroâ€

â†’ âŒ Too vague â†’ ignore location

â€œaround my officeâ€

â†’ âŒ Not explicit â†’ ignore

â€œwithin Indiaâ€

â†’ Still geographic constraint but non-local
â†’ Treat as:

location_match_mode: "global"

â€œhybrid / remote preferredâ€

â†’ global

12ï¸âƒ£ Validation Rules (MANDATORY)

Reject sample if:

location_match_mode missing

Mode contradicts query (e.g., route but only one city)

Exclusion overlaps with target_location

Location inferred, not stated

Mode outside ENUM

ğŸ”’ FINAL LOCK (SAVE THIS)
Location is query-driven.
Mode is mandatory.
No inference.
No geography resolution.
No creativity.

###
FIELD SPECIFICATION: reasoning
1ï¸âƒ£ Definition

reasoning is a post-hoc factual justification of the final extracted output.

It explains WHAT was extracted and WHY,
NOT HOW the model thought.

Reasoning â‰  Chain-of-Thought
Reasoning â‰  Hidden deliberation
Reasoning â‰  Model thinking process

It is descriptive, not procedural.

2ï¸âƒ£ When this field MUST be populated

âœ… ALWAYS populated
No exceptions.

This is mandatory for:

SFT (Supervised Fine-Tuning)

PEFT (LoRA)

Validation

Drift detection

3ï¸âƒ£ When this field MUST be empty

ğŸš« NEVER empty

If reasoning is missing â†’ INVALID SAMPLE

4ï¸âƒ£ Allowed Structure & Data Type
"reasoning": "<single paragraph string>"


Rules:

Exactly one paragraph

Plain English

Neutral, factual tone

Past tense preferred

2â€“5 sentences (recommended)

5ï¸âƒ£ Standardization & Normalization Rules

Reference only extracted facts

Mention only fields that were populated

Do NOT restate schema keys

Do NOT add new facts

Do NOT mention rules, questions, or logic trees

6ï¸âƒ£ What the model MUST do

âœ… Describe:

Intent + subintent decision

Why certain items were extracted

Why constraints were placed as min/max/range

Why location mode was selected (if relevant)

Why exclusions were applied (if any)

âœ… Keep language deterministic

â€œwas classified asâ€¦â€

â€œwas extracted asâ€¦â€

â€œwas treated asâ€¦â€

7ï¸âƒ£ What the model MUST NEVER do (CRITICAL)

ğŸš« NEVER include step-by-step thinking

ğŸš« NEVER include words like:

â€œI thoughtâ€

â€œI analyzedâ€

â€œThen I decidedâ€

â€œFirst / second / finallyâ€

â€œBecause it might meanâ€¦â€

â€œI inferredâ€

â€œPossiblyâ€

ğŸš« NEVER explore alternatives

ğŸš« NEVER justify ambiguity

ğŸš« NEVER explain rules or COT questions

ğŸš« NEVER leak internal logic

If ANY of these appear â†’ REJECT SAMPLE

8ï¸âƒ£ Positive Examples (TRUE POSITIVES)
âœ… Example 1 â€” Product

Query

â€œLooking for a second hand iPhone under 30k, good conditionâ€

Reasoning

â€œThe query was classified as a product purchase because the user is seeking to buy a tangible item. An iPhone was extracted as the item type, with condition marked as used and a maximum price constraint extracted from the stated budget. The condition requirement was captured as a categorical attribute. No location was explicitly mentioned, so local matching applies.â€

âœ… Example 2 â€” Service

Query

â€œNeed a yoga instructor who can come home in the morningsâ€

Reasoning

â€œThe query was classified as a service request because the user is seeking professional assistance. Yoga instruction was extracted as the service item, with home visit mode and morning availability captured from explicit wording. No constraints were applied to pricing or experience. Since no location was specified, proximity-based matching applies.â€

âœ… Example 3 â€” Mutual

Query

â€œLooking for a trekking partner for weekendsâ€

Reasoning

â€œThe query was classified as a mutual intent because it involves shared participation between individuals. Trekking was identified as the subject of connection, with weekend availability extracted as a preference. No exclusions or location details were specified, so local matching applies.â€

9ï¸âƒ£ Negative Examples
âŒ Chain-of-Thought (HARD NEGATIVE)

â€œFirst I checked whether this was a product or service. Then I noticed the word â€˜lookingâ€™, so I assumed the user wants to hire someone. After that Iâ€¦â€

âŒ INVALID
(Leaking reasoning process)

âŒ False Positive (Invented Explanation)

â€œThe user probably wants an experienced person, so experience was assumed.â€

âŒ INVALID
(Inference not stated in query)

âŒ False Negative (Too Vague)

â€œThe user wants something.â€

âŒ INVALID
(Does not justify extraction)

ğŸ”Ÿ Edge Cases & Ambiguity Handling
Ambiguous query

â€œNeed help with my laptopâ€

Allowed reasoning

â€œThe query was classified as a service request because the user is seeking assistance. Laptop repair was extracted as the service item based on the context of help. No specific constraints or preferences were mentioned.â€

âœ… Do NOT mention ambiguity resolution
âœ… Do NOT mention alternative interpretations

11ï¸âƒ£ Validation Checks (MANDATORY)

Reject sample if:

Reasoning includes procedural words (first, then, etc.)

Reasoning mentions â€œinferenceâ€, â€œguessâ€, â€œassumeâ€

Reasoning introduces facts not in output

Reasoning explains rules or questions

Reasoning length > 1 paragraph

Reasoning contradicts extracted fields

ğŸ”’ FINAL LOCK (SAVE THIS)
Reasoning explains the final output.
It never explains how the model decided.
It never thinks aloud.0 data 
### SEMANTIC UNDERSTANDING & CANONICALIZATION (CRITICAL)

----------------------------------------------------------------
CORE PRINCIPLE: SEMANTIC UNDERSTANDING, NOT KEYWORD MATCHING
----------------------------------------------------------------

The model MUST understand MEANING, not match keywords.

Different words with SAME meaning â†’ SAME canonical output
Same word with DIFFERENT meaning â†’ DIFFERENT output based on context

This enables deterministic SQL matching.

----------------------------------------------------------------
HIERARCHICAL ATTRIBUTE EXTRACTION (IMPLICATION RULES)
----------------------------------------------------------------

When attribute X IMPLIES attribute Y, extract BOTH.

Rule: Child attributes ALWAYS imply parent attributes.

ITEM CONDITION HIERARCHY:
```
used (PARENT)
â”œâ”€â”€ single owner â†’ condition: "used" + ownership: "single"
â”œâ”€â”€ second owner â†’ condition: "used" + ownership: "second"
â”œâ”€â”€ multiple owners â†’ condition: "used" + ownership: "multiple"
â”œâ”€â”€ first owner â†’ condition: "used" + ownership: "first"
â””â”€â”€ (unspecified) â†’ condition: "used" only

new (PARENT)
â”œâ”€â”€ sealed/unopened â†’ condition: "new" + packaging: "sealed"
â”œâ”€â”€ open box â†’ condition: "new" + packaging: "open-box"
â””â”€â”€ (unspecified) â†’ condition: "new" only

refurbished (PARENT)
â”œâ”€â”€ certified refurbished â†’ condition: "refurbished" + certification: "certified"
â”œâ”€â”€ seller refurbished â†’ condition: "refurbished" + certification: "seller"
â””â”€â”€ (unspecified) â†’ condition: "refurbished" only
```

EXAMPLES:
| User Says | Understands | Extracts |
|-----------|-------------|----------|
| "single owner car" | used + ownership info | condition: "used", ownership: "single" |
| "used car" | pre-owned | condition: "used" |
| "second hand bike" | pre-owned | condition: "used" |
| "old laptop" | pre-owned | condition: "used" |
| "purana phone" | pre-owned (Hindi) | condition: "used" |
| "pre-owned watch" | pre-owned | condition: "used" |
| "2nd hand furniture" | pre-owned | condition: "used" |
| "first owner bike" | used + first owner | condition: "used", ownership: "first" |

MATCHING LOGIC:
- Buyer searches "used car" â†’ SQL: WHERE condition = 'used' â†’ Returns ALL used cars (including single owner)
- Buyer searches "single owner car" â†’ SQL: WHERE condition = 'used' AND ownership = 'single' â†’ Returns ONLY single owner

----------------------------------------------------------------
VEHICLE-SPECIFIC HIERARCHIES
----------------------------------------------------------------

FUEL TYPE:
```
fuel (PARENT)
â”œâ”€â”€ petrol â†’ fuel: "petrol"
â”œâ”€â”€ diesel â†’ fuel: "diesel"
â”œâ”€â”€ electric â†’ fuel: "electric"
â”œâ”€â”€ hybrid â†’ fuel: "hybrid"
â”œâ”€â”€ cng â†’ fuel: "cng"
â””â”€â”€ lpg â†’ fuel: "lpg"
```

TRANSMISSION:
```
transmission (PARENT)
â”œâ”€â”€ manual â†’ transmission: "manual"
â”œâ”€â”€ automatic â†’ transmission: "automatic"
â”œâ”€â”€ amt â†’ transmission: "amt"
â”œâ”€â”€ cvt â†’ transmission: "cvt"
â””â”€â”€ dct â†’ transmission: "dct"
```

BODY TYPE:
```
vehicle_type (PARENT - for cars)
â”œâ”€â”€ sedan â†’ vehicle_type: "sedan"
â”œâ”€â”€ suv â†’ vehicle_type: "suv"
â”œâ”€â”€ hatchback â†’ vehicle_type: "hatchback"
â”œâ”€â”€ muv â†’ vehicle_type: "muv"
â””â”€â”€ coupe â†’ vehicle_type: "coupe"
```

----------------------------------------------------------------
REAL ESTATE HIERARCHIES
----------------------------------------------------------------

FURNISHING:
```
furnishing (PARENT)
â”œâ”€â”€ furnished â†’ furnishing: "furnished"
â”œâ”€â”€ semi-furnished â†’ furnishing: "semi-furnished"
â””â”€â”€ unfurnished â†’ furnishing: "unfurnished"
```

PROPERTY TYPE:
```
property_type (PARENT)
â”œâ”€â”€ apartment/flat â†’ property_type: "apartment"
â”œâ”€â”€ independent house â†’ property_type: "house"
â”œâ”€â”€ villa â†’ property_type: "villa"
â”œâ”€â”€ plot â†’ property_type: "plot"
â””â”€â”€ pg/hostel â†’ property_type: "pg"
```

BHK EXTRACTION:
| User Says | Extracts |
|-----------|----------|
| "2BHK flat" | bedrooms: 2, property_type: "apartment" |
| "3BHK apartment" | bedrooms: 3, property_type: "apartment" |
| "1RK" | bedrooms: 1, property_type: "apartment", layout: "rk" |

----------------------------------------------------------------
PERSON ATTRIBUTE HIERARCHIES
----------------------------------------------------------------

DIET:
```
diet (PARENT)
â”œâ”€â”€ vegetarian/veg â†’ diet: "vegetarian"
â”œâ”€â”€ non-vegetarian/non-veg â†’ diet: "non-vegetarian"
â”œâ”€â”€ vegan â†’ diet: "vegan"
â”œâ”€â”€ eggetarian â†’ diet: "eggetarian"
â””â”€â”€ jain â†’ diet: "jain"
```

LIFESTYLE FLAGS:
```
habits (BINARY FLAGS ONLY)
â”œâ”€â”€ smoking: "yes" | "no"
â”œâ”€â”€ drinking: "yes" | "no"
â”œâ”€â”€ pets: "yes" | "no"
â””â”€â”€ early_riser: "yes" | "no"
```

SEMANTIC EXPANSIONS:
| User Says | Expands To |
|-----------|------------|
| "no bad habits" | smoking: "no", drinking: "no" |
| "clean lifestyle" | smoking: "no", drinking: "no" |
| "teetotaler" | drinking: "no" |
| "non-smoker" | smoking: "no" |
| "pet-friendly" | pets: "yes" |

----------------------------------------------------------------
UNIVERSAL CANONICALIZATION RULES (PATTERN-BASED)
----------------------------------------------------------------

The model CANNOT memorize all possible values - infinite exist in the world.
Instead, it learns PATTERNS for extracting, standardizing, and normalizing.

CORE PRINCIPLE:
The model learns HOW to identify and process attributes, NOT specific lists.
This makes it FUTURE-PROOF for any unseen product/service/attribute.

----------------------------------------------------------------
CATEGORICAL vs NUMERIC CLASSIFICATION
----------------------------------------------------------------

For EVERY attribute in a query, ask:
"Can this attribute have INFINITE measurable values?"

YES â†’ NUMERIC
- Goes to min/max/range with axis
- Examples: price, storage, experience, age, area, odometer

NO â†’ CATEGORICAL
- Goes to categorical: { key: value }
- Examples: condition, fuel, color, brand, gender, diet, transmission

----------------------------------------------------------------
CATEGORICAL ATTRIBUTES
----------------------------------------------------------------

Definition: Finite discrete choices that cannot be measured with numbers

Structure:
"categorical": {
  "<key>": "<value>"
}

KEY RULES:
- Must be market-standard term
- Lowercase always
- No underscores (use compound nouns if needed)
- Examples: condition, fuel, transmission, furnishing, color, brand

VALUE RULES:
- Canonical form, lowercase
- Standardize synonyms to single term
- Market-recognized choices

EXAMPLES:
| Query Says | Key | Value |
|------------|-----|-------|
| "second hand phone" | condition | used |
| "diesel car" | fuel | diesel |
| "automatic transmission" | transmission | automatic |
| "SSD laptop" | drive | ssd |
| "semi-furnished flat" | furnishing | semi-furnished |
| "red color" | color | red |
| "Apple iPhone" | brand | apple |

----------------------------------------------------------------
NUMERIC ATTRIBUTES
----------------------------------------------------------------

Definition: Measurable quantities with units that map to axes

Structure: min/max/range with axis array
"min": { "<axis>": [{ "type": "", "value": <num>, "unit": "" }] }
"max": { "<axis>": [{ "type": "", "value": <num>, "unit": "" }] }
"range": { "<axis>": [{ "type": "", "min": <num>, "max": <num>, "unit": "" }] }

CONSTRAINT DETECTION (SEMANTIC, NOT KEYWORD):
----------------------------------------------------------------

DEFAULT RULE: A value WITHOUT any modifier is EXACT.
The model must SEMANTICALLY understand the constraint, not match keywords.

| Query | Semantic Meaning | Structure |
|-------|------------------|-----------|
| "128GB" | exactly 128 (no modifier = exact) | range: { min=128, max=128 } |
| "128GB storage" | exactly 128 (no modifier = exact) | range: { min=128, max=128 } |
| "exactly 128GB" | exactly 128 (explicit exact) | range: { min=128, max=128 } |
| "under 50k" | less than 50k | max: { value=50000 } |
| "below 30000" | less than 30000 | max: { value=30000 } |
| "budget 2 lakhs" | at most 2 lakhs | max: { value=200000 } |
| "within 5km" | at most 5km | max: { value=5 } |
| "at least 3 years" | 3 years or more | min: { value=36 } (months) |
| "minimum 5 star" | 5 star or more | min: { value=5 } |
| "above 1500 sqft" | more than 1500 | min: { value=1500 } |
| "15000 km done" | exactly 15000 (stated fact) | range: { min=15000, max=15000 } |
| "asking 85k" | exactly 85k (stated price) | range: { min=85000, max=85000 } |
| "between 20-30 lakhs" | 20 to 30 range | range: { min=2000000, max=3000000 } |
| "10 to 15 years" | 10 to 15 range | range: { min=120, max=180 } (months) |

----------------------------------------------------------------
FUZZY/APPROXIMATE QUANTITY HANDLING (DETERMINISTIC)
----------------------------------------------------------------

When user expresses APPROXIMATE values, convert to RANGE with Â±20% tolerance:

| Query | Interpretation | Structure |
|-------|----------------|-----------|
| "around 5 years" | 4-6 years (Â±20%) | range: { min=48, max=72, unit="months" } |
| "about 50k" | 40k-60k (Â±20%) | range: { min=40000, max=60000 } |
| "roughly 3kg" | 2.4-3.6kg (Â±20%) | range: { min=2.4, max=3.6, unit="kg" } |
| "approximately 100km" | 80-120km (Â±20%) | range: { min=80, max=120, unit="km" } |

FUZZY KEYWORDS â†’ Apply Â±20% range:
- "around", "about", "roughly", "approximately", "nearly", "close to", "ish" (e.g., "50ish")

PRECISE KEYWORDS â†’ Exact value (min=max):
- "exactly", "precisely", no modifier, stated fact

----------------------------------------------------------------
MULTIPLIER EXPANSION (DETERMINISTIC)
----------------------------------------------------------------

Numeric shorthand multipliers are ALLOWED to expand (not inference):

| Shorthand | Expansion | Example |
|-----------|-----------|---------|
| k, K | Ã—1,000 | "50k" â†’ 50000 |
| lakh, lac, L | Ã—100,000 | "2 lakh" â†’ 200000 |
| crore, cr, C | Ã—10,000,000 | "1 crore" â†’ 10000000 |
| M, million | Ã—1,000,000 | "5M" â†’ 5000000 |

NOTE: Currency TYPE (INR/USD/EUR) still requires explicit mention or context.
Multiplier expansion is deterministic; currency inference is NOT.

SEMANTIC UNDERSTANDING (NOT keyword matching):
- "128GB" = user wants EXACTLY 128GB (no flexibility stated)
- "under 50k" = user wants LESS THAN 50k (upper bound)
- "at least 8GB" = user wants 8GB OR MORE (lower bound)
- "5 years experience" = user has/wants EXACTLY 5 years

**CRITICAL: EXACT DOES NOT EXIST AS A FIELD. Exact = range with min=max**
**CRITICAL: No modifier = EXACT (default behavior)**

AXIS MAPPING (10 FIXED AXES - NEVER CHANGES):
| Attribute Type | Axis |
|----------------|------|
| price, budget, salary, cost | cost |
| RAM, storage, rooms, seats | capacity |
| speed, odometer, mileage, refresh rate | performance |
| rating, grade, condition level | quality |
| count, number, quantity | quantity |
| age, experience, duration, usage | time |
| area, distance, dimensions | space |
| gender, profession, certification | identity |
| delivery mode, service mode | mode |
| certifications, proficiency level | skill |

----------------------------------------------------------------
CANONICALIZATION PROCESS (HOW MODEL WORKS)
----------------------------------------------------------------

Step 1: EXTRACT - Identify attribute from query
Step 2: CLASSIFY - Categorical or numeric?
Step 3: STANDARDIZE - Key/value or axis/type to market terms
Step 4: NORMALIZE - Units (if numeric)
Step 5: PLACE - Correct structure in schema

SEMANTIC EQUIVALENCE EXAMPLES (Model learns PATTERNS, not lists):

Condition synonyms â†’ "used":
- "second hand" â†’ used
- "pre-owned" â†’ used
- "purana" (Hindi) â†’ used
- "old" (in item context) â†’ used
- "2nd hand" â†’ used

Fuel synonyms:
- "petrol" / "gasoline" â†’ petrol (India context)
- "diesel" / "gasoil" â†’ diesel

Storage type:
- "SSD" / "Solid State" â†’ ssd
- "HDD" / "Hard Disk" â†’ hdd

The model GENERALIZES from patterns, NOT memorizes lists.

----------------------------------------------------------------
IMPLICATION RULES (Child implies Parent)
----------------------------------------------------------------

When a CHILD attribute is stated, the PARENT must also be extracted.

OWNERSHIP â†’ CONDITION:
| User Says | Extracts |
|-----------|----------|
| "single owner" | condition: "used", ownership: "single" |
| "first owner" | condition: "used", ownership: "first" |
| "second owner" | condition: "used", ownership: "second" |
| "used car" | condition: "used" (no ownership) |

PACKAGING â†’ CONDITION:
| User Says | Extracts |
|-----------|----------|
| "sealed box" | condition: "new", packaging: "sealed" |
| "open box" | condition: "new", packaging: "open-box" |

----------------------------------------------------------------
MATCHING LOGIC (Specific vs Vague)
----------------------------------------------------------------

Extraction determines matching. More specific = narrower SQL match.

RULE: Extract ONLY what is stated.

| User Says | Extracts | SQL Matches |
|-----------|----------|-------------|
| "used car" | condition: "used" | ALL used cars |
| "single owner car" | condition: "used", ownership: "single" | ONLY single owner |
| "car" | (no condition) | ALL cars (new + used) |
| "laptop with SSD" | drive: "ssd" | ONLY SSD laptops |
| "laptop" | (no drive) | ALL laptops |

The extraction is DETERMINISTIC.
The matching is NATURAL consequence of SQL WHERE clauses.

----------------------------------------------------------------
GENERALIZATION (FUTURE-PROOF)
----------------------------------------------------------------

FIXED ELEMENTS (Never changes):
- 14 fields
- 10 axes
- Schema structure
- Constraint model (min/max/range)

FLEXIBLE ELEMENTS (Model generalizes):
- Item types (any market noun)
- Attribute keys (any market-standard categorical)
- Attribute types within axes (any measurable attribute)
- Values (standardized from query)

EXAMPLE - UNSEEN ITEM:
Query: "Selling my quantum computer with 1000 qubits"
```json
{
  "type": "quantum computer",
  "min": {
    "capacity": [
      { "type": "qubits", "value": 1000, "unit": "count" }
    ]
  }
}
```
- "quantum computer" = new type (valid market noun)
- "qubits" = new attribute type (follows pattern)
- Schema remains identical

EXAMPLE - UNSEEN ATTRIBUTE:
Query: "Looking for electric scooter with 100km range"
```json
{
  "type": "scooter",
  "categorical": { "fuel": "electric" },
  "min": {
    "performance": [
      { "type": "range", "value": 100, "unit": "km" }
    ]
  }
}
```

----------------------------------------------------------------
ITEMS STRUCTURE (COMPLETE)
----------------------------------------------------------------

```json
"items": [
  {
    "type": "<canonical market noun>",
    "categorical": {
      "<key>": "<value>"
    },
    "min": {
      "<axis>": [{ "type": "", "value": <number>, "unit": "" }]
    },
    "max": {
      "<axis>": [{ "type": "", "value": <number>, "unit": "" }]
    },
    "range": {
      "<axis>": [{ "type": "", "min": <number>, "max": <number>, "unit": "" }]
    }
  }
]
```

KEY RULES:
- type: Canonical market noun (what the item IS)
- categorical: Non-numeric attributes
  - key: market-standard term (lowercase, no underscores)
  - value: standardized choice (lowercase)
- min/max/range: Numeric attributes mapped to 10 axes

CATEGORICAL vs NUMERIC:
- categorical: Finite discrete choices (condition, fuel, color, brand)
- min/max/range: Measurable quantities (price, storage, experience, age)

EXACT VALUES (NUMERIC):
- Use range with min = max
- Example: "exactly 16GB RAM"
```json
"range": {
  "capacity": [{ "type": "memory", "min": 16, "max": 16, "unit": "gb" }]
}
```

----------------------------------------------------------------
VALIDATION FOR SEMANTIC EXTRACTION
----------------------------------------------------------------

A sample is VALID only if:
1. Implied parent attributes are extracted (single owner â†’ condition: used + ownership: single)
2. Categorical keys/values are market-standard terms (lowercase, no underscores)
3. Synonyms are canonicalized to standard terms
4. Schema structure is preserved (14 fields, 10 axes)
5. Semantic understanding demonstrated (not keyword matching)
6. All stated attributes are extracted (nothing skipped)
7. No unstated attributes are added (no inference)

A sample is INVALID if:
1. Parent attribute missing when child is present
2. Non-standard key/value used (invented terms)
3. Keyword matched instead of semantically understood
4. Schema structure violated (new fields/axes)
5. Stated attribute missing from output
6. Unstated attribute added (inference detected)

### EXTRACTION FLOW SUMMARY

```
STEP 1: CLASSIFY (Q1-Q5)
â†’ Determine intent + sub_intent

STEP 2: LOCATE (Q6-Q11)
â†’ Extract location, mode, exclusions

STEP 3: CATEGORIZE (Q12-Q14)
â†’ Assign domain OR mutual_category

STEP 4: EXTRACT ITEMS (Q15-Q19)
â†’ Items, attributes, item_exclusions

STEP 5: EXTRACT PEOPLE (Q20-Q22)
â†’ other_party_preferences, self_attributes

STEP 6: NORMALIZE (Q23-Q24)
â†’ Convert units, resolve polysemy

STEP 7: VALIDATE (Q25-Q26)
â†’ Check all 12 fields, valid JSON
```


### 
NORMALIZATION & UNIT CONVERSION (Q23-Q24)

**Covers**: All numeric fields across schema

**IMPORTANT**: Units are CONTEXT-DEPENDENT, not fixed. The model learns to REASON about appropriate units based on domain and attribute type.

```
Q23: Determine category and apply appropriate normalization:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ STEP 1: Is this UNIVERSAL or COUNTRY-SPECIFIC?                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â–¼                                           â–¼
          UNIVERSAL                                 COUNTRY-SPECIFIC
    (Physics doesn't change)                    (Value changes by country)
               â”‚                                           â”‚
               â–¼                                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    Preserve Type Always!
    â”‚ STEP 2: Needs        â”‚                    â€¢ Currency: {"max": 5000, "currency": "USD"}
    â”‚ Generalization?      â”‚                    â€¢ Clothing: {"size": "8", "system": "US"}
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â€¢ Shoe: {"size": "10", "system": "US"}
               â”‚                                â€¢ Grade: {"grade": "3.5", "system": "GPA"}
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼
       YES            NO
    (Multiple     (Already Global
     units exist)  Standard)
        â”‚             â”‚
        â–¼             â–¼
    NORMALIZE      KEEP AS-IS
    to standard    (Industry std)
        â”‚             â”‚
        â–¼             â–¼
    100000m â†’ 100km   pixels: "1080p"
    1024 miles â†’ 1638km   carats: 2
    5 years â†’ 60 months   BHP: 150
    1 TB â†’ 1024 GB        Mbps: 100

# GLOBAL REFERENCE CONTEXT â€” VRIDDHI
(Read-only Â· Injected into EVERY stage Â· Never edited during generation)

This document defines the permanent, system-level truths.
It contains NO TASKS and NO PROMPTS.

Any output that violates this file is INVALID.

----------------------------------------------------------------
CORE PHILOSOPHY (LOCKED)
----------------------------------------------------------------

The model extracts facts.
The system standardizes and canonicalizes.
The model NEVER assumes, invents, infers, or converts implicitly.

Strict separation:

User Query
â†’ Extraction (LLM, deterministic)
â†’ Normalization / Standardization (rule-based system)
â†’ Canonicalization (post-processing)
â†’ Matching

If this boundary is violated, the system becomes non-deterministic.

----------------------------------------------------------------
MODEL RESPONSIBILITIES (DETERMINISTIC EXTRACTION)
----------------------------------------------------------------

The model's ONLY job is to:

1. EXTRACT
   - Identify ALL attributes mentioned in query
   - Don't add what's not stated
   - Don't skip what is stated

2. STANDARDIZE
   - Key: canonical attribute category (lowercase, market term)
   - Value: canonical attribute value (lowercase, market term)
   - Examples: "second hand" â†’ "used", "SSD" â†’ "ssd"

3. NORMALIZE
   - Convert units to standard: 3 years â†’ 36 months
   - Convert storage: 1TB â†’ 1024GB
   - Preserve currency type (don't convert USD to INR)

4. OUTPUT IN FIXED SCHEMA
   - 14 fields (fixed)
   - 10 axes (fixed)
   - categorical{} for non-numeric
   - min/max/range{} for numeric
   - NO new fields, NO creativity

MODEL MUST NEVER:
âœ— Infer attributes not stated in query
âœ— Guess missing values
âœ— Create new fields or axes
âœ— Add subjective interpretation
âœ— Be creative with key/value naming
âœ— Skip stated attributes

----------------------------------------------------------------

   UNIVERSAL + NEEDS GENERALIZATION (normalize to standard unit):

    Different expressions, SAME actual value - normalize for comparison:
    â”‚ Distance: 100000 meters, 1024 miles, 50 km â†’ all to KM
    â”‚ Weight: 500 grams, 2 pounds, 5 kg â†’ all to KG
    â”‚ Storage: 1 TB, 512 MB, 256 GB â†’ all to GB
    â”‚ Time: 5 years, 60 months, 1825 days â†’ all to MONTHS
    â”‚ Area: 1000 sqft, 2 acres, 500 sqm â†’ all to SQM
    â”‚
    â”‚ â†’ Output: {"min": 100} (just the normalized number)

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    UNIVERSAL + GLOBAL STANDARD (keep as-is, no conversion needed):

    Industry uses these units globally - no alternative units exist:
    â”‚ Display: pixels (1080p, 4K), inches (screen size)
    â”‚ Jewelry: carats (nobody says "0.4 gram diamond")
    â”‚ Vehicles: BHP/HP, CC/L (same worldwide)
    â”‚ Internet: Mbps/Gbps (same worldwide)
    â”‚ Camera: megapixels (same worldwide)
    â”‚
    â”‚ â†’ Output: {"resolution": "4K"} or {"power": 150} (keep original unit)

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    COUNTRY-SPECIFIC (preserve type - values differ by country):

    Same number means DIFFERENT things in different countries:
    â”‚ Currency: 5000 USD â‰  5000 INR â‰  5000 EUR
    â”‚ Clothing Size: US 8 â‰  UK 8 â‰  EU 38
    â”‚ Shoe Size: US 10 â‰  UK 9 â‰  EU 44
    â”‚ Education: 3.5 GPA (US) â‰  85% (India) â‰  First Class (UK)
    â”‚
    â”‚ â†’ Output: {"max": 5000, "currency": "USD"}
    â”‚ â†’ Output: {"size": "8", "system": "US"}

    "5 years experience" â†’ 60 months
    "60 months experience" â†’ 60 months
    "1825 days experience" â†’ 60 months
    ALL become {"min": 60} â†’ They can MATCH!

    STANDARD UNITS BY ATTRIBUTE TYPE:

    EXPERIENCE/DURATION â†’ MONTHS (standard)
    - 1 year = 12 months
    - 1 week = 0.25 months
    - 1 day = 0.033 months

    DEADLINE/URGENCY â†’ HOURS (standard)
    - 1 day = 24 hours
    - 1 week = 168 hours
    - 1 month = 720 hours

    AGE â†’ YEARS (standard)
    - Keep as years (natural measurement)

    STORAGE â†’ GB (standard)
    - 1 TB = 1024 GB
    - 1 MB = 0.001 GB
    - 1 PB = 1048576 GB

    CURRENCY â†’ BASE UNIT + CURRENCY LABEL (preserve both!)
    - Normalize amount: k = Ã—1000, M = Ã—1000000, lakh = Ã—100000, crore = Ã—10000000
    - PRESERVE currency type: USD, EUR, INR, AED, GBP, JPY, etc.
    - Format: {"max": 50000, "currency": "USD"}
    - If no currency mentioned: {"max": 50000} (infer from location or omit)

    DISTANCE â†’ KILOMETERS (standard)
    - 1 mile = 1.6 km
    - 1 meter = 0.001 km

    AREA â†’ SQUARE METERS/SQM (standard)
    - 1 sqft = 0.093 sqm
    - 1 acre = 4047 sqm
    - 1 hectare = 10000 sqm

    WEIGHT â†’ KILOGRAMS (standard)
    - 1 gram = 0.001 kg
    - 1 pound = 0.45 kg
    - 1 ton = 1000 kg

    PROFICIENCY â†’ 1-5 SCALE (standard)
    - fresher/novice â†’ 1
    - beginner/basic â†’ 2
    - intermediate/experienced â†’ 3
    - advanced/expert/proficient â†’ 4
    - master/guru/specialist â†’ 5

    DOMAIN-SPECIFIC (keep as-is):
    - Jewelry: carats (industry standard)
    - Vehicles: BHP/HP, CC/L
    - Display: inches, pixels
    - Internet: Mbps/Gbps

Q24: Handle POLYSEMY (same word, different meanings based on context):

    WHAT IS POLYSEMY?
    Same word can mean different things in different contexts.
    The model uses DOMAIN + INTENT + WHO context to determine:
    1. What the word MEANS in this context
    2. WHERE it goes in the schema

    HOW TO RESOLVE + WHERE IT GOES:

    "language":
    â”œâ”€â”€ Tech domain + about code â†’ programming language
    â”‚   WHERE: items[].attributes: {code: ["python", "rust"]}
    â”œâ”€â”€ About OTHER person (seller/provider/partner) â†’ speaking language
    â”‚   WHERE: other_party_preferences: {language: "spanish"}
    â””â”€â”€ About SELF ("I speak...") â†’ speaking language
        WHERE: self_attributes: {language: "english"}

    Example: "developer who speaks Spanish and knows Python"
    â†’ other_party_preferences: {language: "spanish"}
    â†’ items[].attributes: {code: ["python"]}
    (BOTH can exist in same query!)

    "size":
    â”œâ”€â”€ Tech domain (storage context)
    â”‚   WHERE: items[].attributes: {storage: 256}
    â”œâ”€â”€ Fashion domain (clothing context)
    â”‚   WHERE: items[].attributes: {size: "XL"}
    â””â”€â”€ Real Estate domain (area context)
        WHERE: items[].attributes: {area: {"min": 93}}

    "experience":
    â”œâ”€â”€ Time-based ("5 years experience", "60 months exp")
    â”‚   WHERE: other_party_preferences OR self_attributes (based on WHO)
    â”‚   VALUE: {experience: {"min": 60}} (normalized to months)
    â””â”€â”€ Skill-based ("experienced person", "expert level")
        WHERE: other_party_preferences OR self_attributes (based on WHO)
        VALUE: {proficiency: {"min": 3}} OR {proficiency: {"min": 4}}
    âš ï¸ NEVER convert between them - they are DIFFERENT dimensions!

    "condition":
    â”œâ”€â”€ Product domain â†’ item condition
    â”‚   WHERE: items[].attributes: {condition: "excellent"}
    â””â”€â”€ Health/Medical context â†’ health condition
        WHERE: context-specific field

    KEY PRINCIPLE:
    These are EXAMPLES to teach the model the PATTERN:
    1. Identify the ambiguous word
    2. Use CONTEXT (domain, intent, who) to determine meaning
    3. Route to correct FIELD based on what/who it describes
```
Q26: VALIDATE JSON structure:
    All 12 fields present?
    Valid JSON syntax?
    Reasoning field explains classification?

----------------------------------------------------------------
1. LOCKED 14-FIELD OUTPUT SCHEMA (FROZEN)
----------------------------------------------------------------

Classification (ALWAYS present):
1. intent
2. subintent
3. domain
4. primary_mutual_category

Extraction:
5. items
6. item_exclusions
7. other_party_preferences
8. other_party_exclusions
9. self_attributes
10. self_exclusions
11. target_location
12. location_match_mode
13. location_exclusions
14. reasoning

No extra fields allowed.
No renaming.
No reordering.
Empty arrays / objects are VALID.

----------------------------------------------------------------
2. INTENTS (ENUM Â· FIXED)
----------------------------------------------------------------

intent âˆˆ { product | service | mutual }

----------------------------------------------------------------
3. SUBINTENTS (ENUM Â· FIXED)
----------------------------------------------------------------

product â†’ buy | sell  
service â†’ seek | provide  
mutual  â†’ connect

----------------------------------------------------------------
4. LOCATION MATCH MODES (ENUM Â· FIXED)
----------------------------------------------------------------

near_me | explicit | target_only | route | global

----------------------------------------------------------------
5. LOCATION RULES (QUERY-DRIVEN ONLY)
----------------------------------------------------------------

â€¢ No location mentioned â†’ near_me  
â€¢ Explicit place name â†’ explicit  
â€¢ Relocating / moving â†’ target_only  
â€¢ Travel from X to Y â†’ route  
â€¢ Remote / anywhere / online â†’ global  

The system MAY inject app/GPS location later.
The model NEVER assumes location.

----------------------------------------------------------------
6. ATTRIBUTE AXES (CLOSED SET Â· NEVER CHANGES)
----------------------------------------------------------------

Every extracted fact MUST map to exactly one axis.

identity  
capacity  
performance  
quality  
quantity  
time  
space  
cost  
mode  
skill  

No new axes ever.

----------------------------------------------------------------
7. ATTRIBUTE TYPE NAMING RULES (NO CREATIVITY)
----------------------------------------------------------------

Type MUST be:
â€¢ Single word
â€¢ Lowercase
â€¢ Market-standard noun
â€¢ No underscores
â€¢ No invention

Examples:
price, experience, storage, memory, battery,
speed, refresh, rating, age, duration

----------------------------------------------------------------
8. CONSTRAINT MODEL (ABSOLUTE)
----------------------------------------------------------------

Exact DOES NOT exist.

Exact = min = max.

Allowed blocks ONLY:
min | max | range

----------------------------------------------------------------
9. CONSTRAINT SHAPE (FROZEN)
----------------------------------------------------------------

min / max:
{
  "<axis>": [
    {
      "type": "<market term>",
      "value": <number|string>,
      "unit": "<standard | local | omitted>"
    }
  ]
}

range:
{
  "<axis>": [
    {
      "type": "<market term>",
      "min": <number>,
      "max": <number>,
      "unit": "<standard | omitted>"
    }
  ]
}

âŒ Axis MUST NEVER appear outside min/max/range.

----------------------------------------------------------------
10. NORMALIZATION & STANDARDIZATION RULES
----------------------------------------------------------------

Normalization happens AFTER extraction.
Extraction NEVER assumes convertibility.

A value MAY be normalized ONLY IF:
1) Axis is known
2) Value is explicit in query
3) Conversion is lossless & deterministic
4) Unit meaning is globally stable

----------------------------------------------------------------
11. STANDARDIZABLE AXES (MINIMUM GUARANTEE)
----------------------------------------------------------------

Convertible (standardize to base unit):

â€¢ time     â†’ month
â€¢ distance â†’ meter
â€¢ area     â†’ sqm
â€¢ weight   â†’ kg
â€¢ storage  â†’ byte (gb allowed)

Universal (NEVER converted):
â€¢ pixels
â€¢ hz
â€¢ count
â€¢ boolean

----------------------------------------------------------------
12. COUNTRY-SPECIFIC / NON-CONVERTIBLE
----------------------------------------------------------------

â€¢ currency  â†’ preserve if explicit, else unit=local
â€¢ education â†’ preserve label exactly
â€¢ legal     â†’ preserve term exactly

âŒ NEVER infer currency
âŒ NEVER convert currency
âŒ NEVER normalize education or legal labels

----------------------------------------------------------------
13. NEW / UNKNOWN UNITS (FUTURE-PROOF RULE)
----------------------------------------------------------------

If axis is known but unit is unknown:
â€¢ Preserve unit verbatim
â€¢ Do NOT convert
â€¢ Do NOT reject

Example:
â€œspeed 5 warp-unitsâ€
â†’ axis: performance
â†’ type: speed
â†’ value: 5
â†’ unit: warp-units

Schema NEVER changes.

----------------------------------------------------------------
14. NO-ASSUMPTION RULE (ABSOLUTE)
----------------------------------------------------------------

If NOT stated in the query:
â€¢ Do not guess
â€¢ Do not infer
â€¢ Do not convert
â€¢ Do not enrich

Empty is VALID.
Silence is VALID.

----------------------------------------------------------------
15. REASONING (NOT CHAIN-OF-THOUGHT)
----------------------------------------------------------------

Purpose:
â€¢ SFT alignment
â€¢ Deterministic justification

Rules:
â€¢ Single paragraph
â€¢ Descriptive, post-hoc
â€¢ Explains WHAT was extracted and WHY
â€¢ No step-by-step thinking
â€¢ No alternatives
â€¢ No hidden deliberation

Chain-of-Thought is FORBIDDEN.

----------------------------------------------------------------
FINAL INVARIANT (LOCK THIS)
----------------------------------------------------------------

Everything in the world reduces to:
â€¢ a fixed semantic axis
â€¢ a market-standard type
â€¢ an explicit value
â€¢ an optional unit
â€¢ a constraint (min | max | range)

Exact does not exist.
Inference does not exist.
Only facts exist. 